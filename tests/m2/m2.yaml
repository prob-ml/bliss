defaults:
  - _self_
  - override hydra/job_logging: stdout

# completely disable hydra logging
# https://github.com/facebookresearch/hydra/issues/910
hydra:
  output_subdir: null
  run:
    dir: .

mode: train

gpus: 1 # use a single gpu by default.

paths:
  root: ${oc.env:BLISS_HOME}
  output: ${paths.root}/output
  sdss: ${paths.root}/data/sdss
  data: ${paths.root}/data

datasets:
    simulated_m2:
        _target_: bliss.datasets.simulated.SimulatedDataset
        prior: ${models.prior}
        decoder: ${models.decoder}
        n_batches: 10
        batch_size: 20
        generate_device: "cuda:0"
        testing_file: null

models:
    decoder:
        n_bands: 2
        slen: 100
        tile_slen: 2
        ptile_slen: 26
        border_padding: 3
        psf_params_file: data/sdss/2583/2/136/psField-002583-2-0136.fits
        prob_galaxy: 0.0
        background_values:
          - 686.0
          - 1123.0
        sdss_bands:
          - 2
          - 3
    encoder:
        n_bands: ${models.decoder.n_bands}
        tile_slen: ${models.decoder.tile_slen}
        ptile_slen: 8
        max_detections: 2
        channel: 17
        spatial_dropout: 0.11399
        dropout: 0.013123
        hidden: 185
    prior: 
        n_bands: 2
        slen: 100
        tile_slen: 2
        max_sources: 5
        mean_sources: 0.48
        min_sources: 0
        f_min: 1e3
        f_max: 1e6
        alpha: 0.5
        prob_galaxy: 0.0
    sleep:
        _target_: bliss.sleep.SleepPhase
        encoder: ${models.encoder}
        prior: ${models.prior}
        decoder: ${models.decoder}

training:
    model: "${models.sleep}"
    dataset: "${datasets.simulated_m2}"
    optimizer:
        name: Adam
        kwargs:
            lr: 0.00098352
            weight_decay: 0.000080410
    n_epochs: 400
    experiment: default
    version: null
    save_top_k: 1
    trainer:
      _target_: pytorch_lightning.Trainer
      logger: False
      checkpoint_callback: False
      profiler: null
      reload_dataloaders_every_epoch: False
      check_val_every_n_epoch: 50
      max_epochs: ${training.n_epochs}
      min_epochs: ${training.n_epochs}
      gpus: ${gpus}
      log_every_n_steps: 10
      deterministic: False

tuning:
    model: "{models.sleep}"
    dataset: "${datasets.simulated_m2}"
    optimizer: ${training.optimizer}
    search_space:
      channel:
        - 8
        - 19
      hidden:
        - 32
        - 257
      spatial_dropout:
        - 0
        - 0.5
      dropout:
        - 0
        - 0.5
      lr:
        - 1e-7
        - 1e-3
      weight_decay:
        - 1e-6
        - 1e-2
    n_epochs: 400
    gpus_per_trial: 1
    allocated_gpus: 4
    grace_period: 10
    n_samples: 500
    verbose: 1
    seed: 42
    save: True
    limit_val_batches: 1
    best_config_save_path: ${paths.root}/config/tuning/m2_best_result.yaml
    log_path: ${paths.root}/output/tuning