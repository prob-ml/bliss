{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import fitsio \n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './../')\n",
    "import simulated_datasets_lib\n",
    "import starnet_lib\n",
    "import plotting_utils\n",
    "import psf_transform_lib\n",
    "\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('torch version: ', torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(453)\n",
    "_ = torch.manual_seed(786)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the PSF I fitted using ground truth Hubble locations/fluxes. \n",
    "init_psf_params = torch.Tensor(np.load('../data/fitted_powerlaw_psf_params.npy'))\n",
    "power_law_psf = psf_transform_lib.PowerLawPSF(init_psf_params.to(device))\n",
    "psf = power_law_psf.forward().detach()\n",
    "\n",
    "# number of bands. Here, there are two. \n",
    "n_bands = psf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "with open('../data/default_star_parameters.json', 'r') as fp:\n",
    "    data_params = json.load(fp)\n",
    "    \n",
    "print(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set background \n",
    "background = torch.zeros(n_bands, data_params['slen'], data_params['slen'])\n",
    "background[0] = 686.\n",
    "background[1] = 1123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw data \n",
    "n_images = 1\n",
    "\n",
    "simulated_dataset = \\\n",
    "    simulated_datasets_lib.load_dataset_from_params(psf,\n",
    "                    data_params,\n",
    "                    background = background,\n",
    "                    n_images = n_images,\n",
    "                    transpose_psf = False, \n",
    "                    add_noise = True)\n",
    "\n",
    "images = simulated_dataset.images.detach()\n",
    "\n",
    "# images is n_images x n_bands x slen x slen\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(images[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_encoder = starnet_lib.StarEncoder(slen = data_params['slen'],\n",
    "                                            patch_slen = 8,\n",
    "                                            step = 2,\n",
    "                                            edge_padding = 3, \n",
    "                                            n_bands = n_bands,\n",
    "                                            max_detections = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_encoder.load_state_dict(torch.load('../example_starnet_encoder',\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "star_encoder.eval(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting samples from the approximate posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the most generic level, the star encoder takes in an image, and returns a set of varational parameters. We can then sample from the variational distribution.  \n",
    "\n",
    "This is done using the `sample_star_encoder` method.\n",
    "\n",
    "We demonstrate this right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_sampled, fluxes_sampled, n_stars_sampled, _, _, _ = \\\n",
    "        star_encoder.sample_star_encoder(image = images, \n",
    "                                     n_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sampled number of stars\n",
    "print(n_stars_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locs_sampled is of shape n_samples x max(n_stars_sampled) x 2 \n",
    "locs_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fluxes_sampled is of shape n_samples x max(n_stars_sampled) x n_bands\n",
    "locs_sampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those rows with less than `max(n_stars_sampled)` number of stars, \"empty\" stars have zeros in the entries for locations and fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As an example, lets take a look at the parameters from the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid overplotting, we'll just look at the bright stars \n",
    "which_bright = torch.log10(fluxes_sampled[0, :, 0]) > 4.5\n",
    "\n",
    "plt.matshow(images[0, 0])\n",
    "plt.scatter(locs_sampled[0, which_bright, 1] * (data_params['slen'] - 1), \n",
    "            locs_sampled[0, which_bright, 0] * (data_params['slen'] - 1), \n",
    "           marker = 'x', color = 'red', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can also return the map estimate, rather than samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_map, fluxes_map, n_stars_map, _, _, _ = \\\n",
    "        star_encoder.sample_star_encoder(image = images, \n",
    "                                         return_map_n_stars = True,\n",
    "                                        return_map_star_params = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, let us zoom in on a subimage. \n",
    "# I have a handy function, \n",
    "\n",
    "# we randomly choose some coordinates to look at\n",
    "x0 = int(np.random.choice(data_params['slen'], 1))\n",
    "x1 = int(np.random.choice(data_params['slen'], 1))\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# the map estimates in red, and truth in blue\n",
    "_ = plotting_utils.plot_subimage(axarr[0], \n",
    "                             images[0, 0], \n",
    "                             locs_map[0], \n",
    "                             simulated_dataset.locs[0], \n",
    "                             x0, x1, \n",
    "                             patch_slen = 10, \n",
    "                            add_colorbar = True, \n",
    "                             global_fig = fig)\n",
    "\n",
    "# here, I'm plotting posterior samples\n",
    "_ = plotting_utils.plot_subimage(axarr[1], \n",
    "                             images[0, 0], \n",
    "                             locs_sampled.view(-1, 2), \n",
    "                             simulated_dataset.locs[0], \n",
    "                             x0, x1, \n",
    "                             patch_slen = 10, \n",
    "                            add_colorbar = True, \n",
    "                             global_fig = fig, \n",
    "                                alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the hood: tiling and un-tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go into some detail now, about the pieces that go into the `sample_star_encoder` method. \n",
    "\n",
    "The way our variational distribution works is, we take the large 100 x 100 image and create image patches. \n",
    "\n",
    "Our variational distribution factorizes over 2 x 2 tiles. \n",
    "\n",
    "To construct a variational distribution on a 2 x 2 tile, we feed the 2 x 2 tile, plus some border, into a neural network. \n",
    "\n",
    "The patch size was is passed to the `patch_slen` argument in the `__init__` when defining the star encoder. The size of the border is set by `edge_padding`. \n",
    "\n",
    "The method `get_image_patches` breaks up the full image, and returns the patches to be inputed in to the neural netowrk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patches, patch_locs, patch_fluxes, patch_n_stars, _ = star_encoder.get_image_patches(images, \n",
    "                                           locs = simulated_dataset.locs, \n",
    "                                           fluxes = simulated_dataset.fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image patches has shape n_patches x n_bands x patch_slen x patch_slen \n",
    "# patch_slen was set in the __init__ of the encoder. \n",
    "\n",
    "image_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method also takes in (optionally) the ground truth locations and fluxes. These are originally in the parameterization for the full image; this function will then give you the parameterization of the locations and fluxes on each individual patch (`patch_locs`, `patch_fluxes`). This is needed for the sleep phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets print an image patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just pick one \n",
    "indx = 1837 #  np.random.choice(image_patches.shape[0])\n",
    "plt.matshow(image_patches[indx, 0])\n",
    "\n",
    "\n",
    "# the neural network returns locations / fluxes for any stars in the center of the grid\n",
    "plt.axvline(x=star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axvline(x=star_encoder.patch_slen - star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axhline(y=star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axhline(y=star_encoder.patch_slen - star_encoder.edge_padding - 0.5, color = 'r')\n",
    "\n",
    "\n",
    "# lets mark any true stars in this particular tile \n",
    "if patch_n_stars[indx] > 0: \n",
    "    _loc = patch_locs[indx][0:patch_n_stars[indx]]\n",
    "    # you have to scale and shift appropriately so the location plots in the right place. \n",
    "    # all locations, whether on the full image or the tile, is parameterized to be between 0 and 1\n",
    "    _loc = _loc * (star_encoder.patch_slen - 2 * star_encoder.edge_padding) +  star_encoder.edge_padding - 0.5\n",
    "    plt.scatter(_loc[:, 1], _loc[:, 0], marker = 'o', color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method takes in image patches and returns variational distribution parameters on each tile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass through the neural network is implemnted by ``get_var_params_all''\n",
    "var_params_all = star_encoder.get_var_params_all(image_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a giant mess of a tensor, giving the triangular array of variational parameters for each patch. \n",
    "\n",
    "For each patch, we have variational parameters for locations \n",
    "\n",
    "$\\{\\ell_{n, i}, f_{n, i}\\}$ for $i = 1, ..., n; n = 1, ..., N_{max}$, \n",
    "\n",
    "as well as the $N_{max}$ probabilities for the number of stars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing into this triangular array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the sleep phase, we need to evaluate the variational distribution at the true number of stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to return the variational parameters that correspond to the true number of stars. \n",
    "\n",
    "# recall that patch_n_stars was the true number of stars on each patch. \n",
    "\n",
    "loc_mean, loc_logvar, \\\n",
    "    log_flux_mean, log_flux_logvar = \\\n",
    "        star_encoder.get_var_params_for_n_stars(var_params_all, \n",
    "                                        n_stars=patch_n_stars.clamp(max = star_encoder.max_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the parameters in an arbirary patch\n",
    "indx = 1837 #  np.random.choice(image_patches.shape[0])\n",
    "\n",
    "# loc_mean has shape n_patches x max_detections x 2\n",
    "print(loc_mean.shape)\n",
    "\n",
    "# as before, ``empty\" stars have zero entries. \n",
    "print(loc_mean[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same plot as before, but I also mark my estimated star on this star\n",
    "\n",
    "plt.matshow(image_patches[indx, 0])\n",
    "\n",
    "# the neural network returns locations / fluxes for any stars in the center of the grid\n",
    "plt.axvline(x=star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axvline(x=star_encoder.patch_slen - star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axhline(y=star_encoder.edge_padding - 0.5, color = 'r')\n",
    "plt.axhline(y=star_encoder.patch_slen - star_encoder.edge_padding - 0.5, color = 'r')\n",
    "\n",
    "\n",
    "# lets mark any true stars in this particular tile \n",
    "if patch_n_stars[indx] > 0: \n",
    "    _loc = patch_locs[indx][0:patch_n_stars[indx]]\n",
    "    # you have to scale and shift appropriately so the location plots in the right place. \n",
    "    # all locations, whether on the full image or the tile, is parameterized to be between 0 and 1\n",
    "    _loc = _loc * (star_encoder.patch_slen - 2 * star_encoder.edge_padding) +  star_encoder.edge_padding - 0.5\n",
    "    plt.scatter(_loc[:, 1], _loc[:, 0], marker = 'o', color = 'b')\n",
    "    \n",
    "# plot my estimated star\n",
    "if patch_n_stars[indx] > 0: \n",
    "    _loc = loc_mean[indx][0:patch_n_stars[indx]].detach()\n",
    "    # you have to scale and shift appropriately so the location plots in the right place. \n",
    "    # all locations, whether on the full image or the tile, is parameterized to be between 0 and 1\n",
    "    _loc = _loc * (star_encoder.patch_slen - 2 * star_encoder.edge_padding) +  star_encoder.edge_padding - 0.5\n",
    "    plt.scatter(_loc[:, 1], _loc[:, 0], marker = 'x', color = 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in the wake phase we need to sample from the variational distribution on n_stars. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the distribution for n_stars on each patch using `get_logprob_n_from_var_params`\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "# log_probs_n has shape n_patches x max_detections\n",
    "log_probs_nstar_patch = star_encoder.get_logprob_n_from_var_params(var_params_all)\n",
    "\n",
    "# these are the sampled number of stars on each patch, of shape\n",
    "# n_samples x n_patches\n",
    "patch_n_stars_sampled = \\\n",
    "        utils.sample_class_weights(torch.exp(log_probs_nstar_patch.detach()), n_samples).view(n_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_mean, loc_logvar, \\\n",
    "    log_flux_mean, log_flux_logvar = \\\n",
    "        star_encoder.get_var_params_for_n_stars(var_params_all,\n",
    "                                                # so the drawn variational parameters depend on n_stars\n",
    "                                                # n_stars tell us which rows of the triangular array to index\n",
    "                                                n_stars = patch_n_stars_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now loc_mean has an extra dimension, corresponding to the number of samples\n",
    "print(loc_mean.shape)\n",
    "\n",
    "# we can then draw samples from loc_mean, loc_logvar, etc. \n",
    "# they are all just Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating the Galaxy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our variational distribution, instead of returning a mean and variance for fluxes, \n",
    "# will return a mean and variance for galaxy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_dim = 8\n",
    "galaxy_encoder = starnet_lib.StarEncoder(\n",
    "                            slen = 100, \n",
    "                            n_bands = 2,\n",
    "                            # patches may need to be larger now\n",
    "                            # and corresponding padding parameters should change\n",
    "                            patch_slen = 8, \n",
    "                            step = 2, \n",
    "                            edge_padding = 3, \n",
    "                            max_detections = 2, \n",
    "                            # the number of source parameters \n",
    "                            n_source_params = gal_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent variables for ** full image **\n",
    "\n",
    "# locations\n",
    "locs = simulated_dataset.locs \n",
    "\n",
    "# I'm just making up galaxy parameters right now. \n",
    "# for nonzero stars, simulate galaxy parameters\n",
    "galaxy_params = torch.zeros(1, simulated_dataset.max_stars, gal_dim)\n",
    "galaxy_params[0, 0:simulated_dataset.n_stars[0], :] = torch.rand(simulated_dataset.n_stars[0], gal_dim)\n",
    "\n",
    "print(galaxy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full image to image patches\n",
    "# convert parameters on full image to parameters on patches \n",
    "\n",
    "image_patches, patch_locs, patch_galaxy_params, patch_n_stars, _ = \\\n",
    "    galaxy_encoder.get_image_patches(images, \n",
    "                                        locs = simulated_dataset.locs, \n",
    "                                        # I still need to chage variable names. \n",
    "                                        # fluxes = galaxy_params\n",
    "                                        fluxes = galaxy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all variational parameters on image patches\n",
    "var_params_all = galaxy_encoder.get_var_params_all(image_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variational parameters for n_sources\n",
    "log_probs_nstar_patch = galaxy_encoder.get_logprob_n_from_var_params(var_params_all)\n",
    "\n",
    "# get variational parameters at true number of stars \n",
    "loc_mean, loc_logvar, \\\n",
    "    galaxy_param_mean, galaxy_param_logvar = \\\n",
    "        galaxy_encoder.get_var_params_for_n_stars(var_params_all, \n",
    "                                        # we clip at max detections\n",
    "                                        n_stars=patch_n_stars.clamp(max = galaxy_encoder.max_detections))\n",
    "\n",
    "# both location and galaxies paremeters are normal with a mean and variance \n",
    "# note that the **log-variance** is returned. Take exp to give variance\n",
    "\n",
    "# Take a note! Off stars are marked with zero. taking exp of zero is not zero :(\n",
    "# in my source code, you'll often see a matrix called is_on_array -- this is just a binary vector \n",
    "# with 1 if star is on and 0 if star is off. \n",
    "# premultiplying by this array sovles this problem. \n",
    "# see get_is_on_from_n_stars_2d and get_is_on_from_n_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# galaxy_param_mean has shape n_patches x max_detections x gal_dim\n",
    "print(galaxy_param_mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wake phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from variational distribution \n",
    "locs_sampled, gal_params_sampled, n_stars_sampled, _, _, _ = \\\n",
    "        galaxy_encoder.sample_star_encoder(images, n_samples = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you would then take these sampled n_stars, locs, galaxy parameters, and pass it into a generative model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_update",
   "language": "python",
   "name": "pytorch_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
