{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../tests/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conftest import *\n",
    "from pathlib import Path\n",
    "\n",
    "paths = dict(data = Path('../data/'))\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "slen = 30\n",
    "tile_slen = 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decoder_setup = DecoderSetup(paths, device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "psf_params = decoder_setup.get_fitted_psf_params()\n",
    "batch_size = 128 if use_cuda else 1\n",
    "n_images = 1280 if use_cuda else 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "star_dataset = decoder_setup.get_star_dataset(\n",
    "        psf_params, n_bands=1, \n",
    "        slen=slen, tile_slen = tile_slen, \n",
    "        max_sources_per_tile=2,\n",
    "        min_sources_per_tile=0,\n",
    "        # this is so that the avg. number of sources \n",
    "        # a 30 x 30 image is 3\n",
    "        mean_sources_per_tile=0.01,\n",
    "        batch_size=batch_size, n_images=n_images\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = star_dataset.get_batch()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.matshow(params['images'].detach().cpu()[0, 0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params['n_sources'].float().mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set up encoder and train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder_setup = EncoderSetup(gpus = 1, device = device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_epochs = 120 if use_cuda else 1\n",
    "trained_encoder = encoder_setup.get_trained_encoder(star_dataset, \n",
    "                                                    n_epochs=n_epochs, \n",
    "                                                    ptile_slen=star_dataset.tile_slen,\n",
    "                                                    tile_slen=star_dataset.tile_slen,\n",
    "                                                    max_detections=star_dataset.max_sources_per_tile\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on saved test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_star = torch.load(paths[\"data\"].joinpath(\"3_star_test.pt\"))\n",
    "test_image = test_star[\"images\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30, 30])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # get the estimated params\n",
    "    trained_encoder.eval()\n",
    "    (\n",
    "        n_sources_per_tile,\n",
    "        locs_per_tile,\n",
    "        galaxy_params_per_tile,\n",
    "        log_fluxes_per_tile,\n",
    "        galaxy_bool_per_tile,\n",
    "    ) = trained_encoder.map_estimate(test_image)\n",
    "    \n",
    "    (\n",
    "        n_sources,\n",
    "        locs,\n",
    "        galaxy_params,\n",
    "        log_fluxes,\n",
    "        galaxy_bool,\n",
    "    ) = trained_encoder.get_full_params_from_sampled_params(n_sources_per_tile,\n",
    "                                                            locs_per_tile,\n",
    "                                                            galaxy_params_per_tile,\n",
    "                                                            log_fluxes_per_tile,\n",
    "                                                            galaxy_bool_per_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_encoder.state_dict(), 'star_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_encoder.load_state_dict(torch.load('star_encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe24f154b50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(test_image[0, 0].cpu())\n",
    "plt.scatter(test_star['locs'][0, :, 1] * slen - 0.5, \n",
    "            test_star['locs'][0, :, 0] * slen - 0.5, color = 'blue')\n",
    "\n",
    "plt.scatter(locs.cpu()[0, :, 1] * slen - 0.5, \n",
    "            locs.cpu()[0, :, 0] * slen - 0.5, color = 'red', marker = 'x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test n_sources and locs\n",
    "assert n_sources == test_star[\"n_sources\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2251, -0.1218],\n",
      "         [ 0.4283,  0.0158],\n",
      "         [ 0.1610,  0.1760]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "diff_locs = test_star[\"locs\"].sort(1)[0].to(device) - locs.sort(1)[0]\n",
    "diff_locs *= test_image.size(-1)\n",
    "print(diff_locs)\n",
    "assert diff_locs.abs().max() <= 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fluxes\n",
    "diff = test_star[\"log_fluxes\"].sort(1)[0].to(device) - log_fluxes.sort(1)[0]\n",
    "assert torch.all(diff.abs() <= log_fluxes.sort(1)[0].abs() * 0.10)\n",
    "assert torch.all(\n",
    "    diff.abs() <= test_star[\"log_fluxes\"].sort(1)[0].abs().to(device) * 0.10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestStarWakeNet:\n",
    "#     @pytest.fixture(scope=\"class\")\n",
    "#     def init_psf_setup(self, decoder_setup, device_setup):\n",
    "#         # initialize psf params, just add 1 to each sigmas\n",
    "#         fitted_psf_params = decoder_setup.get_fitted_psf_params()\n",
    "#         init_psf_params = fitted_psf_params.clone()[None, 0]\n",
    "#         init_psf_params[0, 1:3] += torch.tensor([1.0, 1.0]).to(device_setup.device)\n",
    "#         init_psf = PowerLawPSF(init_psf_params).forward().detach()\n",
    "#         return {\"init_psf_params\": init_psf_params, \"init_psf\": init_psf}\n",
    "\n",
    "#     def test_star_wake(\n",
    "#         self, trained_encoder, star_dataset, init_psf_setup, paths, device_setup\n",
    "#     ):\n",
    "#         # load the test image\n",
    "#         # 3-stars 30*30 pixels.\n",
    "#         test_star = torch.load(paths[\"data\"].joinpath(\"3_star_test.pt\"))\n",
    "#         test_image = test_star[\"images\"]\n",
    "#         test_slen = test_image.size(-1)\n",
    "\n",
    "#         # TODO: Reuse these when creating the background in the fixture\n",
    "#         # initialize background params, which will create the true background\n",
    "#         init_background_params = torch.zeros(1, 3, device=device_setup.device)\n",
    "#         init_background_params[0, 0] = 686.0\n",
    "\n",
    "#         n_samples = 1\n",
    "#         hparams = {\"n_samples\": n_samples, \"lr\": 0.001}\n",
    "#         image_decoder = star_dataset.image_decoder\n",
    "#         assert image_decoder.slen == test_slen\n",
    "#         wake_phase_model = wake.WakeNet(\n",
    "#             trained_encoder,\n",
    "#             image_decoder,\n",
    "#             test_image,\n",
    "#             init_background_params,\n",
    "#             hparams,\n",
    "#         )\n",
    "\n",
    "#         # run the wake-phase training\n",
    "#         n_epochs = 1\n",
    "\n",
    "#         wake_trainer = pl.Trainer(\n",
    "#             gpus=device_setup.gpus,\n",
    "#             profiler=None,\n",
    "#             logger=False,\n",
    "#             checkpoint_callback=False,\n",
    "#             min_epochs=n_epochs,\n",
    "#             max_epochs=n_epochs,\n",
    "#             reload_dataloaders_every_epoch=True,\n",
    "#         )\n",
    "\n",
    "#         wake_trainer.fit(wake_phase_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celeste_py",
   "language": "python",
   "name": "celeste_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
