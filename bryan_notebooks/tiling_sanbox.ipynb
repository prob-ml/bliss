{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytest\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bliss.models import decoder, encoder\n",
    "from bliss import device\n",
    "\n",
    "torch.manual_seed(84)\n",
    "\n",
    "np.random.seed(43)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ptiles_2d(img, tile_shape, step, batch_first=False):\n",
    "    \"\"\"\n",
    "    Take in an image (tensor) and the shape of the padded tile\n",
    "    we want to separate it into and\n",
    "    return the padded tiles also as a tensor.\n",
    "    Taken from: https://gist.github.com/dem123456789/23f18fd78ac8da9615c347905e64fc78\n",
    "    \"\"\"\n",
    "\n",
    "    tile_H, tile_W = tile_shape[0], tile_shape[1]\n",
    "    if img.size(2) < tile_H:\n",
    "        num_padded_H_Top = (tile_H - img.size(2)) // 2\n",
    "        num_padded_H_Bottom = tile_H - img.size(2) - num_padded_H_Top\n",
    "        padding_H = nn.ConstantPad2d((0, 0, num_padded_H_Top, num_padded_H_Bottom), 0)\n",
    "        img = padding_H(img)\n",
    "    if img.size(3) < tile_W:\n",
    "        num_padded_W_Left = (tile_W - img.size(3)) // 2\n",
    "        num_padded_W_Right = tile_W - img.size(3) - num_padded_W_Left\n",
    "        padding_W = torch.nn.ConstantPad2d(\n",
    "            (num_padded_W_Left, num_padded_W_Right, 0, 0), 0\n",
    "        )\n",
    "        img = padding_W(img)\n",
    "    step_int = [0, 0]\n",
    "    step_int[0] = int(tile_H * step[0]) if (isinstance(step[0], float)) else step[0]\n",
    "    step_int[1] = int(tile_W * step[1]) if (isinstance(step[1], float)) else step[1]\n",
    "    ptiles_fold_H = img.unfold(2, tile_H, step_int[0])\n",
    "    if (img.size(2) - tile_H) % step_int[0] != 0:\n",
    "        ptiles_fold_H = torch.cat(\n",
    "            (ptiles_fold_H, img[:, :, -tile_H:,].permute(0, 1, 3, 2).unsqueeze(2)),\n",
    "            dim=2,\n",
    "        )\n",
    "    ptiles_fold_HW = ptiles_fold_H.unfold(3, tile_W, step_int[1])\n",
    "    if (img.size(3) - tile_W) % step_int[1] != 0:\n",
    "        ptiles_fold_HW = torch.cat(\n",
    "            (\n",
    "                ptiles_fold_HW,\n",
    "                ptiles_fold_H[:, :, :, -tile_W:, :].permute(0, 1, 2, 4, 3).unsqueeze(3),\n",
    "            ),\n",
    "            dim=3,\n",
    "        )\n",
    "    ptiles = ptiles_fold_HW.permute(2, 3, 0, 1, 4, 5)\n",
    "    ptiles = ptiles.reshape(-1, img.size(0), img.size(1), tile_H, tile_W)\n",
    "    if batch_first:\n",
    "        ptiles = ptiles.permute(1, 0, 2, 3, 4)\n",
    "    return ptiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tile_images(images, ptile_slen, step):\n",
    "    \"\"\"\n",
    "    Breaks up a large image into smaller padded tiles.\n",
    "    Each tile has size ptile_slen x ptile_slen, where\n",
    "    the number of padded tiles per image  is (slen - ptile_slen / step)**2.\n",
    "    NOTE: input and output are torch tensors.\n",
    "    :param images: A tensor of size (batch_size x n_bands x slen x slen)\n",
    "    :param ptile_slen: The side length of each padded tile.\n",
    "    :return: image_ptiles, output tensor of shape:\n",
    "             (batch_size * ptiles per image) x n_bands x ptile_slen x ptile_slen\n",
    "    :rtype: class:`torch.Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(images.shape) == 4\n",
    "\n",
    "    image_xlen = images.shape[2]\n",
    "    image_ylen = images.shape[3]\n",
    "\n",
    "    # My tile coords doesn't work otherwise ...\n",
    "    assert (image_xlen - ptile_slen) % step == 0\n",
    "    assert (image_ylen - ptile_slen) % step == 0\n",
    "\n",
    "    n_bands = images.shape[1]\n",
    "    image_ptiles = torch.tensor([], device=device)\n",
    "    for b in range(n_bands):\n",
    "        image_ptiles_b = _extract_ptiles_2d(\n",
    "            images[:, b : (b + 1), :, :],\n",
    "            tile_shape=[ptile_slen, ptile_slen],\n",
    "            step=[step, step],\n",
    "            batch_first=True,\n",
    "        ).reshape(-1, 1, ptile_slen, ptile_slen)\n",
    "\n",
    "        # torch.cat(...) works with empty tensors.\n",
    "        image_ptiles = torch.cat((image_ptiles, image_ptiles_b.to(device)), dim=1)\n",
    "\n",
    "    return image_ptiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "slen = 52\n",
    "n_bands = 2\n",
    "\n",
    "ptile_slen = 10\n",
    "tile_slen = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(1, n_bands, slen, slen, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = _tile_images(images, ptile_slen, tile_slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2, 10, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 52, 52])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (slen - (ptile_slen - 1) - 1) / tile_slen + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get new image-tiler: this is part of the encoder now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder.ImageEncoder(\n",
    "                        slen=slen,\n",
    "                        ptile_slen=ptile_slen,\n",
    "                        tile_slen=tile_slen,\n",
    "                        n_bands=n_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = enc.get_images_in_tiles(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(output == output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celeste_py",
   "language": "python",
   "name": "celeste_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
