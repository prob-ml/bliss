{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bliss.surveys.dc2 import DC2DataModule\n",
    "from case_studies.dc2_diffusion.utils.catalog_parser import CatalogParser\n",
    "from bliss.catalog import TileCatalog\n",
    "from bliss.encoder.metrics import CatalogMatcher\n",
    "from case_studies.dc2_diffusion.utils.metrics import DetectionPerformance\n",
    "from bliss.global_env import GlobalEnv\n",
    "\n",
    "from case_studies.dc2_new_diffusion.utils.autoencoder import CatalogEncoder, CatalogDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "with initialize(config_path=\"./latent_diffusion_config\", version_base=None):\n",
    "    new_diffusion_notebook_cfg = compose(\"latent_diffusion_notebook_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_slen = new_diffusion_notebook_cfg.surveys.dc2.tile_slen\n",
    "max_sources_per_tile = new_diffusion_notebook_cfg.surveys.dc2.max_sources_per_tile\n",
    "r_band_min_flux = new_diffusion_notebook_cfg.notebook_var.r_band_min_flux\n",
    "\n",
    "dc2: DC2DataModule = instantiate(new_diffusion_notebook_cfg.surveys.dc2)\n",
    "dc2.batch_size = 512\n",
    "dc2.setup(stage=\"fit\")\n",
    "GlobalEnv.current_encoder_epoch = 1\n",
    "GlobalEnv.seed_in_this_program = 7272\n",
    "dc2_train_dataloader = dc2.train_dataloader()\n",
    "\n",
    "catalog_parser: CatalogParser = instantiate(new_diffusion_notebook_cfg.encoder.catalog_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = catalog_parser.n_params_per_source\n",
    "encoder = CatalogEncoder(target_ch, hidden_dim=32)\n",
    "decoder = CatalogDecoder(target_ch, hidden_dim=32 // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(device=device)\n",
    "decoder = decoder.to(device=device)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train encoder and decoder with image weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "total_batch = len(dc2_train_dataloader)\n",
    "epoch = 3\n",
    "for _ in range(epoch):\n",
    "    i = 0\n",
    "    dc2_train_dataloader = dc2.train_dataloader()\n",
    "    for batch in tqdm.tqdm(dc2_train_dataloader):\n",
    "        batch = move_data_to_device(batch, device=device)\n",
    "        target_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "        target_cat1 = target_cat.get_brightest_sources_per_tile(\n",
    "            band=2, exclude_num=0\n",
    "        )\n",
    "        encoded_catalog_tensor = catalog_parser.encode(target_cat1).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        encoder_pred = encoder(encoded_catalog_tensor)\n",
    "        image_weights = (torch.log(torch.norm(batch[\"images\"], \n",
    "                                              dim=1, p=2, keepdim=True) + 1) * 100) + 1\n",
    "        weighted_encoder_pred = encoder_pred * (1 / image_weights)\n",
    "        recovered_target = decoder(weighted_encoder_pred)\n",
    "        loss = ((recovered_target - encoded_catalog_tensor) ** 2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"step [{i + 1}/{total_batch}], loss: {loss.item():.6f}\")\n",
    "        i += 1\n",
    "    GlobalEnv.current_encoder_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = CatalogMatcher(dist_slack=1.0)\n",
    "f1_metric = DetectionPerformance().to(device=device)\n",
    "dc2_val_dataloader = dc2.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "for batch in tqdm.tqdm(dc2_val_dataloader):\n",
    "    batch = move_data_to_device(batch, device=device)\n",
    "    target_tile_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "    target_tile_cat = target_tile_cat.get_brightest_sources_per_tile(band=2,  exclude_num=0)\n",
    "    target_full_cat = target_tile_cat.to_full_catalog(tile_slen)\n",
    "\n",
    "    encoded_catalog_tensor = catalog_parser.encode(target_tile_cat).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "    with torch.no_grad():\n",
    "        encoder_pred = encoder(encoded_catalog_tensor)\n",
    "        image_weights = (torch.log(torch.norm(batch[\"images\"], \n",
    "                                              dim=1, p=2, keepdim=True) + 1) * 100) + 1  # regularization \n",
    "        weighted_encoder_pred = encoder_pred * (1 / image_weights)\n",
    "        recovered_target = decoder(weighted_encoder_pred)\n",
    "    recovered_target = catalog_parser.clip_tensor(recovered_target.permute([0, 2, 3, 1]))\n",
    "    recovered_tile_cat = catalog_parser.decode(recovered_target)\n",
    "    recovered_full_cat = recovered_tile_cat.to_full_catalog(tile_slen)\n",
    "\n",
    "    matching = matcher.match_catalogs(target_full_cat, recovered_full_cat)\n",
    "    f1_metric.update(target_full_cat, recovered_full_cat, matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in f1_metric.compute().items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((encoder_pred[0].norm(dim=0, p=2) + 1).log().cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a new decoder without image weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder = CatalogDecoder(target_ch, hidden_dim=32 // 4)\n",
    "new_decoder = new_decoder.to(device=device)\n",
    "new_optimizer = optim.Adam(new_decoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "new_decoder.train()\n",
    "\n",
    "total_batch = len(dc2_train_dataloader)\n",
    "epoch = 3\n",
    "for _ in range(epoch):\n",
    "    i = 0\n",
    "    dc2_train_dataloader = dc2.train_dataloader()\n",
    "    for batch in tqdm.tqdm(dc2_train_dataloader):\n",
    "        batch = move_data_to_device(batch, device=device)\n",
    "        target_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "        target_cat1 = target_cat.get_brightest_sources_per_tile(\n",
    "            band=2, exclude_num=0\n",
    "        )\n",
    "        encoded_catalog_tensor = catalog_parser.encode(target_cat1).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "    \n",
    "        new_optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            encoder_pred = encoder(encoded_catalog_tensor)\n",
    "        recovered_target = new_decoder(encoder_pred)\n",
    "        loss = ((recovered_target - encoded_catalog_tensor) ** 2).mean()\n",
    "        loss.backward()\n",
    "        new_optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"step [{i + 1}/{total_batch}], loss: {loss.item():.6f}\")\n",
    "        i += 1\n",
    "    GlobalEnv.current_encoder_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = CatalogMatcher(dist_slack=1.0)\n",
    "f1_metric = DetectionPerformance().to(device=device)\n",
    "dc2_val_dataloader = dc2.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "new_decoder.eval()\n",
    "for batch in tqdm.tqdm(dc2_val_dataloader):\n",
    "    batch = move_data_to_device(batch, device=device)\n",
    "    target_tile_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "    target_tile_cat = target_tile_cat.get_brightest_sources_per_tile(band=2,  exclude_num=0)\n",
    "    target_full_cat = target_tile_cat.to_full_catalog(tile_slen)\n",
    "\n",
    "    encoded_catalog_tensor = catalog_parser.encode(target_tile_cat).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "    with torch.no_grad():\n",
    "        encoder_pred = encoder(encoded_catalog_tensor)\n",
    "        recovered_target = new_decoder(encoder_pred)\n",
    "    recovered_target = catalog_parser.clip_tensor(recovered_target.permute([0, 2, 3, 1]))\n",
    "    recovered_tile_cat = catalog_parser.decode(recovered_target)\n",
    "    recovered_full_cat = recovered_tile_cat.to_full_catalog(tile_slen)\n",
    "\n",
    "    matching = matcher.match_catalogs(target_full_cat, recovered_full_cat)\n",
    "    f1_metric.update(target_full_cat, recovered_full_cat, matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in f1_metric.compute().items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "torch.save(new_decoder.state_dict(), \"decoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((batch[\"images\"][image_index].norm(dim=0, p=2) + 1).log().cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pred.max(), encoder_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((encoder_pred[image_index].norm(dim=0, p=2) + 1).log().cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((1 / (batch[\"images\"][image_index].norm(dim=0, p=2) * 100 + 1)).cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_cat_tensor = F.interpolate(encoded_catalog_tensor, scale_factor=4, mode=\"bilinear\")\n",
    "plt.imshow(upsampled_cat_tensor[image_index, 0].cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recovered_target.permute([0, 3, 1, 2])[image_index, 0].cpu().numpy(), \n",
    "           cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "\n",
    "dc2_train_dataloader = dc2.train_dataloader()\n",
    "encoder_pred_max = -torch.inf\n",
    "encoder_pred_min = torch.inf\n",
    "for batch in tqdm.tqdm(dc2_train_dataloader):\n",
    "    batch = move_data_to_device(batch, device=device)\n",
    "    target_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "    target_cat1 = target_cat.get_brightest_sources_per_tile(\n",
    "        band=2, exclude_num=0\n",
    "    )\n",
    "    encoded_catalog_tensor = catalog_parser.encode(target_cat1).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_pred = encoder(encoded_catalog_tensor)\n",
    "    if encoder_pred.max() > encoder_pred_max:\n",
    "        encoder_pred_max = encoder_pred.max()\n",
    "    if encoder_pred.min() < encoder_pred_min:\n",
    "        encoder_pred_min = encoder_pred.min()\n",
    "\n",
    "dc2_val_dataloader = dc2.val_dataloader()\n",
    "for batch in tqdm.tqdm(dc2_val_dataloader):\n",
    "    batch = move_data_to_device(batch, device=device)\n",
    "    target_cat = TileCatalog(batch[\"tile_catalog\"])\n",
    "    target_cat1 = target_cat.get_brightest_sources_per_tile(\n",
    "        band=2, exclude_num=0\n",
    "    )\n",
    "    encoded_catalog_tensor = catalog_parser.encode(target_cat1).permute([0, 3, 1, 2])  # (b, k, h, w)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_pred = encoder(encoded_catalog_tensor)\n",
    "\n",
    "    if encoder_pred.max() > encoder_pred_max:\n",
    "        encoder_pred_max = encoder_pred.max()\n",
    "    if encoder_pred.min() < encoder_pred_min:\n",
    "        encoder_pred_min = encoder_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pred_min, encoder_pred_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
