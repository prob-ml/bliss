{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335744c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from case_studies.dc2_mdt.utils.rml_df import RMLDF\n",
    "from case_studies.dc2_mdt.utils.new_simulate_image import ImageSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 8\n",
    "max_objects = 2\n",
    "image_normalize_strategy = \"linear_scale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fc210",
   "metadata": {},
   "source": [
    "## RML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rml_loss_mask_fn(x0_population: torch.Tensor, output_population: torch.Tensor):\n",
    "    ns_mask = x0_population[..., 0:1] > 0.0\n",
    "    output_ns = output_population[..., 0:1]\n",
    "    output_other = output_population[..., 1:]\n",
    "    return torch.cat([output_ns, \n",
    "                      torch.where(ns_mask, output_other, torch.full_like(output_other, fill_value=-1.0))],\n",
    "                      dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rml_pred_x0_rectify_fn(pred_x0: torch.Tensor):\n",
    "    ns_mask = pred_x0[..., 0:1] > 0.0\n",
    "    pred_ns = pred_x0[..., 0:1]\n",
    "    pred_other = pred_x0[..., 1:]\n",
    "    return torch.cat([pred_ns, \n",
    "                      torch.where(ns_mask, pred_other, torch.full_like(pred_other, fill_value=-1.0))],\n",
    "                      dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rml_loss_weight_fn(t: torch.Tensor, alpha: torch.Tensor, sigma: torch.Tensor):\n",
    "    alpha_2 = alpha ** 2 + 1e-3\n",
    "    sigma_2 = sigma ** 2 + 1e-3\n",
    "    loss_weights = 1 / (1 + sigma_2 / alpha_2)\n",
    "    return loss_weights.to(device=t.device)[t.flatten()].view(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierMLP(nn.Module):\n",
    "    def __init__(self, data_shape, num_layers, hidden_ch):\n",
    "        super().__init__()\n",
    "        \n",
    "        data_flat_len = int(np.prod(data_shape))\n",
    "\n",
    "        self.register_buffer(\"timestep_coeff\", torch.linspace(start=0.1, end=100, steps=hidden_ch))  # (hidden, )\n",
    "        self.timestep_phase = nn.Parameter(torch.randn(hidden_ch))  # (hidden, )\n",
    "        self.input_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)\n",
    "        )\n",
    "        self.timestep_embed = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_ch, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch),\n",
    "        )\n",
    "        self.image_embed = nn.Sequential(\n",
    "            nn.Linear(image_size * image_size, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)\n",
    "        )\n",
    "        # self.layers = nn.Sequential(\n",
    "        #     nn.Linear(hidden_ch * (data_flat_len + 1) + data_flat_len, hidden_ch * 4), \n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(hidden_ch * 4, hidden_ch * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(hidden_ch * 2, hidden_ch),\n",
    "        #     nn.GELU(),\n",
    "        #     *[\n",
    "        #         nn.Sequential(nn.Linear(hidden_ch, hidden_ch), nn.GELU())\n",
    "        #         for _ in range(num_layers)\n",
    "        #     ],\n",
    "        #     nn.Linear(hidden_ch, data_flat_len),\n",
    "        # )\n",
    "\n",
    "        self.layers_net = nn.ModuleList([\n",
    "            nn.Linear(hidden_ch * (data_flat_len + 1) + data_flat_len, hidden_ch * 2), \n",
    "            nn.GELU(),\n",
    "            # nn.Linear(hidden_ch * 4, hidden_ch * 2),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(hidden_ch * 2, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            *[\n",
    "                nn.Sequential(nn.Linear(hidden_ch, hidden_ch), nn.GELU())\n",
    "                for _ in range(num_layers)\n",
    "            ],\n",
    "            nn.Linear(hidden_ch, data_flat_len),\n",
    "        ])\n",
    "\n",
    "    def layers(self, x):\n",
    "        for i, m in enumerate(self.layers_net):\n",
    "            if i < 4 or i == len(self.layers_net) - 1:\n",
    "                x = m(x)\n",
    "                continue\n",
    "            x = m(x) + x\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t, image, epsilon, is_training):\n",
    "        if is_training:\n",
    "            t = t.unsqueeze(-1)  # (b, m, max_objects, k, 1)\n",
    "            sin_embed_t = torch.sin(\n",
    "                (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "            )\n",
    "            cos_embed_t = torch.cos(\n",
    "                (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "            )\n",
    "            embed_t = self.timestep_embed(\n",
    "                rearrange(torch.stack([sin_embed_t, cos_embed_t], dim=0), \n",
    "                          \"d b m max_objects k hidden -> b m max_objects k (d hidden)\")\n",
    "            )  # (b, m, max_objects, k, hidden)\n",
    "            embed_xt = self.input_embed(x.unsqueeze(-1))  # (b, m, max_objects, k, hidden)\n",
    "            embed_image = self.image_embed(image.flatten(1))  # (b, hidden)\n",
    "            embed_image = repeat(embed_image, \"b hidden -> b m hidden\", m=t.shape[1])\n",
    "            out = self.layers(\n",
    "                torch.cat([(embed_xt + embed_t).flatten(2), \n",
    "                           embed_image, \n",
    "                           epsilon.flatten(2)], dim=-1)\n",
    "            )\n",
    "            return out.view(x.shape)\n",
    "        t = t.clone()\n",
    "        image = image.clone()\n",
    "        t = t.unsqueeze(-1)  # (b, max_objects, k, 1)\n",
    "        sin_embed_t = torch.sin(\n",
    "            (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "        )\n",
    "        cos_embed_t = torch.cos(\n",
    "            (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "        )\n",
    "        embed_t = self.timestep_embed(\n",
    "            rearrange(torch.stack([sin_embed_t, cos_embed_t], dim=0), \n",
    "                      \"d b max_objects k hidden -> b max_objects k (d hidden)\")\n",
    "        )\n",
    "        embed_xt = self.input_embed(x.unsqueeze(-1))\n",
    "        embed_image = self.image_embed(image.flatten(1))\n",
    "        out = self.layers(\n",
    "            torch.cat([(embed_xt + embed_t).flatten(1), \n",
    "                       embed_image, \n",
    "                       epsilon.flatten(1)], dim=-1)\n",
    "        )\n",
    "        return out.view(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\")\n",
    "batch_size = 1024\n",
    "training_time_steps = 1000\n",
    "training_iters = 20_000\n",
    "ddim_eta = 0.0  # use 0.0 for better results when max_objects >= 2\n",
    "log_freq = 500\n",
    "seed = 1201023\n",
    "pytorch_lightning.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_simulator = ImageSimulator(img_height=image_size,\n",
    "                                 img_width=image_size,\n",
    "                                 max_objects=max_objects,\n",
    "                                 psf_stdev=1.0,\n",
    "                                 flux_alpha=10.0,\n",
    "                                 flux_beta=0.01,\n",
    "                                 pad=0,\n",
    "                                 always_max_count=False,\n",
    "                                 constant_locs=False,\n",
    "                                 coadd_images=True).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_diffusion = RMLDF(num_timesteps=training_time_steps,\n",
    "                            m=32,\n",
    "                            lambda_=1.0,\n",
    "                            beta=1.0,\n",
    "                            loss_mask_fn=rml_loss_mask_fn,\n",
    "                            pred_x0_rectify_fn=rml_pred_x0_rectify_fn,\n",
    "                            loss_weight_fn=rml_loss_weight_fn)\n",
    "sampling_diffusion = training_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = FourierMLP(data_shape=[2, 4], \n",
    "                    num_layers=8, \n",
    "                    hidden_ch=64).to(device=device)\n",
    "my_optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-3, amsgrad=True)\n",
    "my_scheduler = torch.optim.lr_scheduler.MultiStepLR(my_optimizer, milestones=[training_iters // 5 * 4], gamma=0.1)\n",
    "# my_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(my_optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_flux_boundary = 2000.0\n",
    "# def encode_flux(flux: torch.Tensor):\n",
    "#     assert flux.min() >= 0.0\n",
    "#     flux = flux.clamp(max=max_flux_boundary)\n",
    "#     return (torch.log1p(flux) / torch.log1p(torch.tensor(max_flux_boundary))) * 2 - 1\n",
    "\n",
    "# def decode_flux(flux_minus1_to_1: torch.Tensor):\n",
    "#     assert flux_minus1_to_1.min() >= -1.0 and flux_minus1_to_1.max() <= 1.0\n",
    "#     return torch.expm1((flux_minus1_to_1 + 1) / 2 * torch.log1p(torch.tensor(max_flux_boundary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb76376",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flux_boundary = 2000.0\n",
    "def encode_flux(flux: torch.Tensor):\n",
    "    assert flux.min() >= 0.0\n",
    "    flux = flux.clamp(max=max_flux_boundary)\n",
    "    return (flux / max_flux_boundary) * 2 - 1\n",
    "\n",
    "def decode_flux(flux_minus1_to_1: torch.Tensor):\n",
    "    assert flux_minus1_to_1.min() >= -1.0 and flux_minus1_to_1.max() <= 1.0\n",
    "    return (flux_minus1_to_1 + 1) / 2 * max_flux_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3338f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_x_start(catalog):\n",
    "    n_sources = catalog[\"counts\"]  # (b, )\n",
    "    locs = catalog[\"locs\"]  # (b, m, 2)\n",
    "    fluxes = catalog[\"fluxes\"].unsqueeze(-1)  # (b, m, 1)\n",
    "    n_sources = (n_sources.unsqueeze(-1) >= torch.arange(1, locs.shape[1] + 1, device=locs.device)).unsqueeze(-1)  # (b, m, 1)\n",
    "    x_start = torch.cat([n_sources * 2 - 1, locs / image_size * 2 - 1, encode_flux(fluxes)], dim=-1)  # (b, m, 4)\n",
    "    dist_to_ori = torch.sqrt(((x_start[..., 1:3] + 1) ** 2).sum(dim=-1))\n",
    "    sorted_index = dist_to_ori.argsort(dim=-1, descending=True)  # (b, m)\n",
    "    return torch.take_along_dim(x_start, repeat(sorted_index, \"... -> ... r\", r=4), dim=-2)  # (b, m, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7842f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_x_start(output_x_start):\n",
    "    n_sources = output_x_start[..., 0] > 0.0  # (b, m)\n",
    "    locs = (output_x_start[..., 1:3] + 1) / 2 * image_size  # (b, m, 2)\n",
    "    fluxes = decode_flux(output_x_start[..., 3])  # (b, m)\n",
    "    return {\n",
    "        \"counts\": n_sources.sum(dim=-1),\n",
    "        \"n_sources\": n_sources.int(),\n",
    "        \"locs\": locs,\n",
    "        \"fluxes\": fluxes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(input_image):\n",
    "    match image_normalize_strategy:\n",
    "        case \"none\":\n",
    "            output_image = input_image\n",
    "        case \"log\":\n",
    "            output_image = torch.log1p(input_image)\n",
    "        case \"linear_scale\":\n",
    "            output_image = input_image / 1000\n",
    "        case _:\n",
    "            raise NotImplementedError()\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_t_schedule(batch_size):\n",
    "    pred_ns_mode = torch.from_numpy(np.random.choice(training_time_steps, size=(batch_size, 1)))\n",
    "    pred_ns_mode = torch.cat([pred_ns_mode,\n",
    "                              torch.full((batch_size, 3), fill_value=training_time_steps - 1, dtype=torch.long)],\n",
    "                              dim=-1)\n",
    "    pred_locs_mode = torch.from_numpy(np.random.choice(training_time_steps, size=(batch_size, 1)))\n",
    "    pred_locs_mode = torch.cat([torch.full((batch_size, 1), fill_value=0, dtype=torch.long),\n",
    "                                pred_locs_mode,\n",
    "                                pred_locs_mode,\n",
    "                                torch.full((batch_size, 1), fill_value=training_time_steps - 1, dtype=torch.long)],\n",
    "                                dim=-1)\n",
    "    pred_fluxes_mode = torch.from_numpy(np.random.choice(training_time_steps, size=(batch_size, 1)))\n",
    "    pred_fluxes_mode = torch.cat([torch.full((batch_size, 3), fill_value=0, dtype=torch.long),\n",
    "                                  pred_fluxes_mode],\n",
    "                                  dim=-1)\n",
    "    t = torch.cat([pred_ns_mode, pred_locs_mode, pred_fluxes_mode], dim=0)\n",
    "    t = t[torch.randperm(t.shape[0])[:batch_size]]\n",
    "    return repeat(t, \"b k -> b m k\", m=max_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aa2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_file_path = Path(f\"./rml_df_model_{training_iters}.pt\")\n",
    "if not saved_file_path.exists():\n",
    "    my_net.train()\n",
    "    loss_record = []\n",
    "    for i in tqdm.tqdm(list(range(training_iters))):\n",
    "        catalog = image_simulator.generate(batch_size)\n",
    "        t = training_t_schedule(batch_size).to(device=device)\n",
    "        input_image = catalog[\"images\"]  # (b, h, w)\n",
    "        input_image = normalize_image(input_image)\n",
    "        train_loss_args = {\n",
    "            \"model\": my_net,\n",
    "            \"x_start\": encode_x_start(catalog),\n",
    "            \"t\": t,\n",
    "        }\n",
    "        loss = training_diffusion.training_losses(**train_loss_args, \n",
    "                                                model_kwargs={\"image\": input_image})[\"loss\"]\n",
    "        loss = loss.mean()\n",
    "        loss_record.append(loss.item())\n",
    "        my_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        my_optimizer.step()\n",
    "        my_scheduler.step()\n",
    "        if (i + 1) % log_freq == 0:\n",
    "            print(f\"[{i + 1}/{training_iters}] loss: {loss.item():.3e}\")    \n",
    "    \n",
    "    torch.save(my_net.state_dict(), saved_file_path)\n",
    "\n",
    "    plt.plot(loss_record)\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "else:\n",
    "    with open(saved_file_path, \"rb\") as f:\n",
    "        my_net_state_dict = torch.load(f, map_location=device)\n",
    "        my_net.load_state_dict(my_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a38752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_t(input_t: torch.Tensor, pad_front_num, pad_rear_num):\n",
    "    assert input_t.ndim == 1\n",
    "    pad_front_v = torch.full((pad_front_num, ), fill_value=input_t[0].item(), device=input_t.device)\n",
    "    pad_rear_v = torch.full((pad_rear_num, ), fill_value=input_t[-1].item(), device=input_t.device)\n",
    "    return torch.cat([pad_front_v, input_t, pad_rear_v], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba767594",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_sample_steps = 5\n",
    "locs_sample_steps = 5\n",
    "fluxes_sample_steps = 3\n",
    "\n",
    "def generate_k_vec(sample_steps):\n",
    "    return torch.linspace(0, training_time_steps - 1, sample_steps).int().flip(dims=(0,))\n",
    "\n",
    "n_sources_k_vec = pad_t(generate_k_vec(ns_sample_steps), \n",
    "                        pad_front_num=0, \n",
    "                        pad_rear_num=locs_sample_steps + fluxes_sample_steps)\n",
    "locs_y_k_vec = pad_t(generate_k_vec(locs_sample_steps), \n",
    "                     pad_front_num=ns_sample_steps, \n",
    "                     pad_rear_num=fluxes_sample_steps)\n",
    "locs_x_k_vec = locs_y_k_vec\n",
    "flux_k_vec = pad_t(generate_k_vec(fluxes_sample_steps), \n",
    "                   pad_front_num=ns_sample_steps + locs_sample_steps, \n",
    "                   pad_rear_num=0)\n",
    "k_matrix = repeat(torch.stack([n_sources_k_vec, locs_y_k_vec, locs_x_k_vec, flux_k_vec], dim=-1),\n",
    "                  \"k f -> k b m f\", b=batch_size, m=max_objects).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net.eval()\n",
    "val_true_cat = []\n",
    "val_est_cat = []\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm.tqdm(list(range(1000))):\n",
    "        val_catalog = image_simulator.generate(batch_size=1024)\n",
    "        input_image = normalize_image(val_catalog[\"images\"])\n",
    "        val_catalog = decode_x_start(encode_x_start(val_catalog))\n",
    "        val_true_cat.append(move_data_to_device(val_catalog, \"cpu\"))\n",
    "        diffusion_sampling_config = {\n",
    "            \"model\": my_net,\n",
    "            \"shape\": (1024, max_objects, 4),\n",
    "            \"clip_denoised\": True,\n",
    "            \"model_kwargs\": {\"image\": input_image}\n",
    "        }\n",
    "        sample = sampling_diffusion.ddim_sample_loop(**diffusion_sampling_config, \n",
    "                                                     k_matrix=k_matrix,\n",
    "                                                     eta=ddim_eta)\n",
    "        val_est_cat.append(move_data_to_device(decode_x_start(sample), \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a50ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_pred_ns = []\n",
    "diffusion_true_ns = []\n",
    "diffusion_pred_locs = []\n",
    "diffusion_true_locs = []\n",
    "diffusion_pred_fluxes = []\n",
    "diffusion_true_fluxes = []\n",
    "for ec, tc in zip(val_est_cat, val_true_cat, strict=True):\n",
    "    diffusion_pred_ns.append(ec[\"n_sources\"])\n",
    "    diffusion_true_ns.append(tc[\"n_sources\"])\n",
    "    diffusion_pred_locs.append(ec[\"locs\"])\n",
    "    diffusion_true_locs.append(tc[\"locs\"])\n",
    "    diffusion_pred_fluxes.append(ec[\"fluxes\"])\n",
    "    diffusion_true_fluxes.append(tc[\"fluxes\"])\n",
    "diffusion_pred_ns = torch.cat(diffusion_pred_ns, dim=0)\n",
    "diffusion_true_ns = torch.cat(diffusion_true_ns, dim=0)\n",
    "diffusion_pred_locs = torch.cat(diffusion_pred_locs, dim=0)\n",
    "diffusion_true_locs = torch.cat(diffusion_true_locs, dim=0)\n",
    "diffusion_pred_fluxes = torch.cat(diffusion_pred_fluxes, dim=0)\n",
    "diffusion_true_fluxes = torch.cat(diffusion_true_fluxes, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_true_ns.shape, diffusion_pred_ns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_pred_locs.shape, diffusion_true_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46efbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_pred_fluxes.shape, diffusion_true_fluxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(d_pred_bin_index, d_true_bin_index, bin_num, bin_labels, axis_label):\n",
    "    d_cm = torch.zeros(bin_num, bin_num, dtype=torch.int)\n",
    "    for ri in range(d_cm.shape[0]):\n",
    "        for ci in range(d_cm.shape[1]):\n",
    "            d_cm[ri, ci] = ((d_pred_bin_index == ri) & (d_true_bin_index == ci)).sum()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    sns.heatmap(d_cm,\n",
    "                annot=True,\n",
    "                fmt=\"d\", cmap=\"Greens\", cbar=False,\n",
    "                xticklabels=bin_labels,\n",
    "                yticklabels=bin_labels,\n",
    "                ax=ax)\n",
    "    ax.set_xlabel(f\"True {axis_label}\")\n",
    "    ax.set_ylabel(f\"Pred {axis_label}\")\n",
    "    ax.set_title(\"Diffusion\")\n",
    "    fig.show()\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    # sns.heatmap(d_cm / torch.sum(d_cm, dim=0, keepdim=True),\n",
    "    #             annot=True,\n",
    "    #             fmt=\".2f\", cmap=\"Greens\", cbar=False,\n",
    "    #             xticklabels=bin_labels,\n",
    "    #             yticklabels=bin_labels,\n",
    "    #             ax=ax)\n",
    "    # ax.set_xlabel(f\"True {axis_label}\")\n",
    "    # ax.set_ylabel(f\"Pred {axis_label}\")\n",
    "    # ax.set_title(\"Diffusion (CM in Percent)\")\n",
    "    # fig.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    sns.heatmap(((d_cm - d_cm.T) / torch.minimum(d_cm, d_cm.T).clamp(min=1)).abs(),\n",
    "                annot=(d_cm - d_cm.T) / torch.minimum(d_cm, d_cm.T).clamp(min=1),\n",
    "                fmt=\".2f\", cmap=\"Greens\", cbar=False,\n",
    "                xticklabels=bin_labels,\n",
    "                yticklabels=bin_labels,\n",
    "                ax=ax)\n",
    "    ax.set_xlabel(f\"True {axis_label}\")\n",
    "    ax.set_ylabel(f\"Pred {axis_label}\")\n",
    "    af = ((d_cm - d_cm.T) / d_cm.sum()).abs().sum() / (bin_num * (bin_num - 1))\n",
    "    ax.set_title(f\"Diffusion (Asymmetry Factor = {af:.2e})\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(diffusion_pred_ns.sum(dim=-1), diffusion_true_ns.sum(dim=-1), \n",
    "        bin_num=3, bin_labels=list(range(3)), axis_label=\"Source Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dad72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inclusive_bucektize(input_t, boundary):\n",
    "    new_boundary = boundary.clone()\n",
    "    new_boundary[0] -= 1e-3\n",
    "    new_boundary[-1] += 1e-3\n",
    "    b_index = torch.bucketize(input_t, new_boundary)\n",
    "    assert (b_index > 0).all()\n",
    "    assert (b_index < new_boundary.shape[0]).all()\n",
    "    return b_index - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_source_mask = diffusion_pred_ns.bool() & \\\n",
    "      (diffusion_pred_ns.sum(dim=-1) == diffusion_true_ns.sum(dim=-1)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc154a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_bin_boundary = torch.linspace(0.0, 8.0, 5)\n",
    "d_pred_locs_x_bin_index = inclusive_bucektize(diffusion_pred_locs[valid_source_mask][:, 1], locs_bin_boundary)\n",
    "d_true_locs_x_bin_index = inclusive_bucektize(diffusion_true_locs[valid_source_mask][:, 1], locs_bin_boundary)\n",
    "plot_cm(d_pred_locs_x_bin_index, d_true_locs_x_bin_index, \n",
    "        bin_num=4, bin_labels=[f\"[{bl1:.1f}, {bl2:.1f}]\" \n",
    "                               for bl1, bl2 in zip(locs_bin_boundary[:-1], \n",
    "                                                   locs_bin_boundary[1:])], \n",
    "        axis_label=\"Loc X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_bin_boundary = torch.linspace(0.0, 8.0, 5)\n",
    "d_pred_locs_y_bin_index = inclusive_bucektize(diffusion_pred_locs[valid_source_mask][:, 0], locs_bin_boundary)\n",
    "d_true_locs_y_bin_index = inclusive_bucektize(diffusion_true_locs[valid_source_mask][:, 0], locs_bin_boundary)\n",
    "plot_cm(d_pred_locs_y_bin_index, d_true_locs_y_bin_index, \n",
    "        bin_num=4, bin_labels=[f\"[{bl1:.1f}, {bl2:.1f}]\" \n",
    "                               for bl1, bl2 in zip(locs_bin_boundary[:-1], \n",
    "                                                   locs_bin_boundary[1:])], \n",
    "        axis_label=\"Loc Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes_bin_boundary = torch.linspace(0.0, 2000.0, 6)\n",
    "d_pred_fluxes_bin_index = inclusive_bucektize(diffusion_pred_fluxes[valid_source_mask], fluxes_bin_boundary)\n",
    "d_true_fluxes_bin_index = inclusive_bucektize(diffusion_true_fluxes[valid_source_mask], fluxes_bin_boundary)\n",
    "plot_cm(d_pred_fluxes_bin_index, d_true_fluxes_bin_index, \n",
    "        bin_num=5, bin_labels=[f\"[{bl1:.1f}, {bl2:.1f}]\" \n",
    "                               for bl1, bl2 in zip(fluxes_bin_boundary[:-1], \n",
    "                                                   fluxes_bin_boundary[1:])], \n",
    "        axis_label=\"Fluxes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
