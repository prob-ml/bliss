{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import pytorch_lightning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from case_studies.dc2_mdt.utils.reverse_markov_learning import RMLDiffusion\n",
    "from case_studies.dc2_mdt.utils.resample import SpeedSampler, SigmoidSampler\n",
    "from case_studies.dc2_mdt.utils.simulate_image import ImageSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 4\n",
    "max_objects = 2\n",
    "image_normalize_strategy = \"linear_scale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79700bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleMLP(nn.Module):\n",
    "#     def __init__(self, layers, hidden_ch):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(1 + 1 + 16, hidden_ch),\n",
    "#             nn.ReLU(),\n",
    "#             *[\n",
    "#                 nn.Sequential(\n",
    "#                     nn.Linear(hidden_ch, hidden_ch),\n",
    "#                     nn.LayerNorm(hidden_ch),\n",
    "#                     nn.ReLU(),\n",
    "#                 ) for _ in range(layers)\n",
    "#             ],\n",
    "#             nn.Linear(hidden_ch, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x, t, image):\n",
    "#         x = torch.cat([x, t.unsqueeze(1), image.flatten(start_dim=1)], dim=1)\n",
    "#         return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierMLP(nn.Module):\n",
    "    def __init__(self, data_shape, num_layers, hidden_ch):\n",
    "        super().__init__()\n",
    "        self.data_shape = [data_shape]\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"timestep_coeff\", torch.linspace(start=0.1, end=100, steps=hidden_ch)[None]\n",
    "        )  # (1, hidden)\n",
    "        self.timestep_phase = nn.Parameter(torch.randn(hidden_ch)[None])  # (1, hidden)\n",
    "        self.input_embed = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(data_shape)), hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)\n",
    "        )\n",
    "        self.timestep_embed = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_ch, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch),\n",
    "        )\n",
    "        self.image_embed = nn.Sequential(\n",
    "            nn.Linear(image_size * image_size, hidden_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hidden_ch * 2 + 1, hidden_ch), \n",
    "            nn.GELU(),\n",
    "            *[\n",
    "                nn.Sequential(nn.Linear(hidden_ch, hidden_ch), nn.GELU())\n",
    "                for _ in range(num_layers)\n",
    "            ],\n",
    "            nn.Linear(hidden_ch, int(np.prod(data_shape))),\n",
    "        )\n",
    "\n",
    "        self.vmap_timestep_embed = torch.vmap(self.timestep_embed.__call__, in_dims=1, out_dims=1, randomness=\"same\")\n",
    "        self.vmap_input_embed = torch.vmap(self.input_embed.__call__, in_dims=1, out_dims=1, randomness=\"same\")\n",
    "        self.vmap_layers = torch.vmap(self.layers.__call__, in_dims=1, out_dims=1, randomness=\"same\")\n",
    "\n",
    "    def forward(self, x, t, image, epsilon, is_training):\n",
    "        if is_training:\n",
    "            t = t.unsqueeze(-1)  # (b, m, 1)\n",
    "            sin_embed_t = torch.sin(\n",
    "                (self.timestep_coeff.unsqueeze(0) * t.float()) + self.timestep_phase.unsqueeze(0)\n",
    "            )\n",
    "            cos_embed_t = torch.cos(\n",
    "                (self.timestep_coeff.unsqueeze(0) * t.float()) + self.timestep_phase.unsqueeze(0)\n",
    "            )\n",
    "            embed_t = self.vmap_timestep_embed(\n",
    "                rearrange(torch.stack([sin_embed_t, cos_embed_t], dim=0), \n",
    "                          \"d b m w -> b m (d w)\")\n",
    "            )  # (b, m, hidden)\n",
    "            embed_xt = self.vmap_input_embed(x.flatten(2))  # (b, m, hidden)\n",
    "            embed_image = self.image_embed(image.flatten(1))  # (b, hidden)\n",
    "            embed_image = repeat(embed_image, \"b hidden -> b m hidden\", m=t.shape[1])\n",
    "            out = self.vmap_layers(\n",
    "                torch.cat([embed_xt + embed_t, embed_image, epsilon], dim=-1)\n",
    "            )\n",
    "            return out.view(x.shape)\n",
    "        \n",
    "        t = t.unsqueeze(-1)  # (b, 1)\n",
    "        sin_embed_t = torch.sin(\n",
    "            (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "        )  # (b, hidden)\n",
    "        cos_embed_t = torch.cos(\n",
    "            (self.timestep_coeff * t.float()) + self.timestep_phase\n",
    "        )\n",
    "        embed_t = self.timestep_embed(\n",
    "            rearrange(torch.stack([sin_embed_t, cos_embed_t], dim=0), \n",
    "                      \"d b w -> b (d w)\")\n",
    "        )\n",
    "        embed_xt = self.input_embed(x.flatten(1))\n",
    "        embed_image = self.image_embed(image.flatten(1))\n",
    "        out = self.layers(\n",
    "            torch.cat([embed_xt + embed_t, embed_image, epsilon], dim=-1)\n",
    "        )\n",
    "        return out.view(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "batch_size = 1024\n",
    "val_batch_size = 100\n",
    "training_time_steps = 1000\n",
    "training_iters = 5000\n",
    "val_iters = 5000\n",
    "ddim_steps = 10\n",
    "ddim_eta = 1.0\n",
    "log_freq = 500\n",
    "seed = 1201023\n",
    "pytorch_lightning.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_diffusion = RMLDiffusion(num_timesteps=training_time_steps,\n",
    "                                  num_sampling_steps=ddim_steps,\n",
    "                                  m=64,\n",
    "                                  lambda_=1.0,\n",
    "                                  beta=0.5)\n",
    "sampling_diffusion = training_diffusion\n",
    "# schedule_sampler = create_named_schedule_sampler(\"uniform\", training_diffusion)\n",
    "# schedule_sampler = SpeedSampler(diffusion=training_diffusion,\n",
    "#                                 lam=0.6,\n",
    "#                                 k=5,\n",
    "#                                 tau=700)\n",
    "schedule_sampler = SigmoidSampler(training_diffusion, 0)\n",
    "image_simulator = ImageSimulator(img_height=image_size,\n",
    "                                 img_width=image_size,\n",
    "                                 max_objects=max_objects,\n",
    "                                 min_flux=200.0,\n",
    "                                 psf_stdev=1.0 if image_size > 1 else 0.1,\n",
    "                                 background_intensity=100.0).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = FourierMLP(data_shape=[1,], \n",
    "                    num_layers=8, \n",
    "                    hidden_ch=256).to(device=device)\n",
    "# my_net = SimpleMLP(layers=8, hidden_ch=256).to(device=device)\n",
    "my_optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-3, amsgrad=True)\n",
    "my_scheduler = torch.optim.lr_scheduler.MultiStepLR(my_optimizer, milestones=[training_iters // 4 * 3], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_flux(flux: torch.Tensor):\n",
    "    assert flux.min() > 0.0 and flux.max() <= 2000.0\n",
    "    return (torch.log1p(flux) / 7.61) * 2 - 1\n",
    "\n",
    "def decode_flux(flux_minus1_to_1: torch.Tensor):\n",
    "    assert flux_minus1_to_1.min() >= -1.0 and flux_minus1_to_1.max() <= 1.0\n",
    "    return torch.expm1((flux_minus1_to_1 + 1) / 2 * 7.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aa2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net.train()\n",
    "loss_record = []\n",
    "for i in tqdm.tqdm(list(range(training_iters))):\n",
    "    catalog = image_simulator.generate(batch_size)\n",
    "    t, batch_sample_weights, batch_loss_weights = schedule_sampler.sample(batch_size, device=device)\n",
    "    target_fluxes = catalog[\"fluxes\"][:, 0:1]  # (b, 1)\n",
    "    input_image = catalog[\"images\"].unsqueeze(1)  # (b, 1, h, w)\n",
    "    match image_normalize_strategy:\n",
    "        case \"none\":\n",
    "            pass\n",
    "        case \"log\":\n",
    "            input_image = torch.log1p(input_image)\n",
    "        case \"linear_scale\":\n",
    "            input_image = input_image / 1000\n",
    "        case _:\n",
    "            raise NotImplementedError()\n",
    "    train_loss_args = {\n",
    "        \"model\": my_net,\n",
    "        \"x_start\": encode_flux(target_fluxes),\n",
    "        \"t\": t,\n",
    "        \"loss_weights\": batch_loss_weights\n",
    "    }\n",
    "    loss = training_diffusion.training_losses(**train_loss_args, \n",
    "                                              model_kwargs={\"image\": input_image})[\"loss\"]\n",
    "    loss = (loss * batch_sample_weights).mean()\n",
    "    loss_record.append(loss.item())\n",
    "    my_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optimizer.step()\n",
    "    my_scheduler.step()\n",
    "    if (i + 1) % log_freq == 0:\n",
    "        print(f\"[{i + 1}/{training_iters}] loss: {loss.item():.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2609ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_record)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net.eval()\n",
    "val_true_fluxes = None\n",
    "val_est_fluxes = []\n",
    "with torch.inference_mode():\n",
    "    val_catalog = image_simulator.generate(batch_size=val_batch_size, \n",
    "                                               seed=seed)\n",
    "    for i in tqdm.tqdm(list(range(val_iters))):\n",
    "        target_fluxes = val_catalog[\"fluxes\"][:, 0:1]  # (b, 1)\n",
    "        if val_true_fluxes is None:\n",
    "            val_true_fluxes = target_fluxes\n",
    "        else:\n",
    "            assert torch.allclose(val_true_fluxes, target_fluxes)\n",
    "        input_image = val_catalog[\"images\"].unsqueeze(1)\n",
    "        match image_normalize_strategy:\n",
    "            case \"none\":\n",
    "                pass\n",
    "            case \"log\":\n",
    "                input_image = torch.log1p(input_image)\n",
    "            case \"linear_scale\":\n",
    "                input_image = input_image / 1000\n",
    "            case _:\n",
    "                raise NotImplementedError()\n",
    "        diffusion_sampling_config = {\n",
    "            \"model\": my_net,\n",
    "            \"shape\": (val_batch_size, 1),\n",
    "            \"clip_denoised\": True,\n",
    "            \"model_kwargs\": {\"image\": input_image}\n",
    "        }\n",
    "        sample = sampling_diffusion.ddim_sample_loop(**diffusion_sampling_config, eta=ddim_eta)\n",
    "        val_est_fluxes.append(decode_flux(sample).cpu())\n",
    "val_est_fluxes = torch.stack(val_est_fluxes, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6545dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_true_fluxes = val_true_fluxes.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_true_fluxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc95a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_est_fluxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((val_true_fluxes - val_est_fluxes) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0593785",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.abs(val_true_fluxes - val_est_fluxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(val_est_fluxes.flatten(), color=\"red\", alpha=0.5, density=True, label=\"Est\")\n",
    "# plt.hist(val_true_fluxes.flatten(), color=\"green\", alpha=0.5, density=True, label=\"True\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.xlabel(\"Flux\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_cover = torch.tensor([i / 100 for i in range(5, 100, 5)])\n",
    "left_q_points = [(1.0 - c) / 2 for c in ci_cover]\n",
    "right_q_points = [1.0 - lq for lq in left_q_points]\n",
    "actual_ci_cover = []\n",
    "for q in zip(left_q_points, right_q_points):\n",
    "    q = torch.tensor(q)\n",
    "    est_fluxes_q = val_est_fluxes.quantile(q=q, dim=0).permute([1, 2, 0])  # (b, 1, 2)\n",
    "    above_lower_bound = val_true_fluxes[:, 0] > est_fluxes_q[:, 0, 0]\n",
    "    below_upper_bound = val_true_fluxes[:, 0] < est_fluxes_q[:, 0, 1]\n",
    "    actual_ci_cover.append((above_lower_bound & below_upper_bound).sum() / val_true_fluxes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f96057",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ci_cover = torch.cat([a.unsqueeze(0) for a in actual_ci_cover])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ci_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "a_m_c = actual_ci_cover - ci_cover\n",
    "ci_sigma = (ci_cover * (1 - ci_cover) / val_batch_size) ** 0.5\n",
    "plt.plot(ci_cover, a_m_c, label=\"Diffusion (RML)\")\n",
    "plt.fill_between(ci_cover, a_m_c - ci_sigma, a_m_c + ci_sigma, alpha=0.3, label=r\"$1\\sigma$\")\n",
    "plt.fill_between(ci_cover, a_m_c - 2 * ci_sigma, a_m_c + 2 * ci_sigma, alpha=0.3, label=r\"$2\\sigma$\")\n",
    "plt.fill_between(ci_cover, a_m_c - 3 * ci_sigma, a_m_c + 3 * ci_sigma, alpha=0.3, label=r\"$3\\sigma$\")\n",
    "plt.plot(ci_cover, torch.zeros_like(ci_cover), linestyle=\"dashed\", label=\"Expected\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.xticks(ci_cover, rotation=45)\n",
    "plt.xlabel(\"Expected CI Cover\")\n",
    "plt.ylabel(\"Actual - True CI Cover\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
