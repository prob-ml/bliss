---
defaults:
    - ../../../bliss/conf@_here_: base_config
    - _self_
    - override hydra/job_logging: stdout

mode: train

dc2_min_flux: 0.0
dc2_min_log_flux: 0.0
# dc2_max_log_flux: 23.43
dc2_max_log_flux: 10.0
# dc2_max_flux: inf
dc2_max_flux: 22025.0

surveys:
  dc2:
    batch_size: 32
    train_transforms:
      - _target_: bliss.cached_dataset.OneBandTransform
        band_idx: 2  # r-band
      - _target_: bliss.data_augmentation.RotateFlipTransform
      - _target_: bliss.data_augmentation.RandomShiftTransform
        tile_slen: ${surveys.dc2.tile_slen}
        max_sources_per_tile: ${surveys.dc2.max_sources_per_tile}
      - _target_: bliss.cached_dataset.FluxFilterTransform
        reference_band: 0  
        min_flux: 100
    nontrain_transforms:
      - _target_: bliss.cached_dataset.OneBandTransform
        band_idx: 2  # r-band
      - _target_: bliss.cached_dataset.FluxFilterTransform
        reference_band: 0  
        min_flux: 100

my_diffusion_factors:
  - _target_: case_studies.dc2_mdt.utils.catalog_parser.OneBitFactor
    n_params: 1
    name: n_sources
    bit_value: 1.0
    threshold: 0.0
    sample_rearrange: b ht wt 1 -> b ht wt
    loss_gating: null
    jitter_scaler: 0.05
  - _target_: case_studies.dc2_mdt.utils.catalog_parser.NormalizedFactor
    n_params: 2
    name: locs
    data_min: 0.0
    data_max: 1.0
    scale: 1.0
    latent_zero_point: -1.0
    sample_rearrange: b ht wt d -> b ht wt 1 d
    loss_gating:
      _target_: case_studies.dc2_mdt.utils.catalog_parser.SourcesGating
    jitter_scaler: 0.05
  - _target_: case_studies.dc2_mdt.utils.catalog_parser.LogNormalizedFactor
    n_params: 1
    name: fluxes
    data_min: ${dc2_min_flux}
    log_data_min: ${dc2_min_log_flux}
    log_data_max: ${dc2_max_log_flux}
    scale: 1.0
    latent_zero_point: -1.0
    sample_rearrange: b ht wt d -> b ht wt 1 d
    loss_gating:
      _target_: case_studies.dc2_mdt.utils.catalog_parser.SourcesGating
    jitter_scaler: 0.05

my_metrics:
  detection_performance:
    _target_: case_studies.dc2_mdt.utils.metrics.DetectionPerformance
  flux_error:
    _target_: bliss.encoder.metrics.FluxError
    survey_bands: ${encoder.survey_bands}
    base_flux_bin_cutoffs: [200, 400, 600, 800, 1000]
    mag_zero_point: 3631e9  # for DC2
    report_bin_unit: mag
    ref_band: 0
  asymmetric_cm:
    _target_: case_studies.dc2_mdt.utils.metrics.AsymmetricCM
    max_n_sources: 2
    locs_bin_boundaries: [0.0, 0.25, 0.5, 0.75, 1.0]
    flux_bin_boundaries: [21.0, 22.2, 23.4, 24.6, 25.8]
    flux_bands: ${encoder.survey_bands}

my_image_normalizers:
  clahe:
    _target_: bliss.encoder.image_normalizer.ClaheNormalizer
    min_stdev: 200
  asinh:
    _target_: bliss.encoder.image_normalizer.AsinhQuantileNormalizer
    q: [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999, 0.9999, 0.99999]

encoder:
    _target_: case_studies.dc2_mdt.utils.flow_matching_encoder.DC2FMEncoder
    survey_bands: [r]
    reference_band: 0
    d_flow_matching_type: vanilla
    acc_grad_batches: 1
    max_fluxes: ${dc2_max_flux}
    optimizer_params:
        lr: 3e-4
        amsgrad: true  # to make adam more stable
    scheduler_params:
        milestones: [700, 790]
        gamma: 0.1
    image_normalizers: ${my_image_normalizers}
    catalog_parser:
        _target_: case_studies.dc2_mdt.utils.catalog_parser.CatalogParser
        factors: ${my_diffusion_factors}
    image_size: [80, 80]
    matcher:
        _target_: bliss.encoder.metrics.CatalogMatcher
        dist_slack: 1.0
        mag_slack: null
        mag_band: 2  # SDSS r-band
    mode_metrics:
        _target_: torchmetrics.MetricCollection
        _convert_: partial
        compute_groups: false
        metrics: ${my_metrics}

train:
    trainer:
        logger:
            name: DC2_mdt_flow_matching_exp
            version: null  # change it before running the code
        devices: null  # change it before running the code
        use_distributed_sampler: false  # disable this because we use the self-defined distributed sampler
        precision: 32-true
        max_epochs: 800
        # detect_anomaly: false
        # gradient_clip_val: 0.0
    data_source: ${surveys.dc2}
    pretrained_weights: null
    seed: 7272
    callbacks:
        early_stopping:
            _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
            monitor: val/_loss
            mode: min
            patience: 800
        schedule_checkpointing:
            _target_: pytorch_lightning.callbacks.ModelCheckpoint
            filename: "schedule_saved_encoder_{epoch:03d}"
            save_top_k: -1
            every_n_epochs: 50
            verbose: true
            save_on_train_epoch_end: true
            auto_insert_metric_name: false