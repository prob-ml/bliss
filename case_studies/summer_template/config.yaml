defaults:
    - _self_
    - override hydra/job_logging: stdout

hydra:
    output_subdir: null
    run:
        dir: .

mode: train

paths:
    root: ${oc.env:BLISS_HOME}
    data: ${paths.root}/data
    sdss: ${paths.data}/sdss
    project: ${paths.root}/case_studies/summer_template
    output: ${paths.project}/output


# TODO: add the ability to use simulated datasets that have been stored on disk
# TODO: make DC2 dataset/simulator with labels and train with it
# TODO: create some multiband datasets
# TODO: create configs files like this one for DECaLS and DC2

simulator:
    _target_: bliss.simulator.simulated_dataset.SimulatedDataset
    prior:
        _target_: bliss.simulator.prior.ImagePrior
        n_bands: 1
        max_sources: 1
        mean_sources: 0.2
        min_sources: 0
        f_min: 622.0
        f_max: 1e6
        alpha: 0.43
        prob_galaxy: 0.30
        galaxy_prior:
            _target_: bliss.simulator.galsim_galaxies.SingleGalsimGalaxyPrior
            flux_sample: "pareto"
            min_flux: 622.0
            max_flux: 1e6
            alpha: 0.47
            a_sample: "gamma"
            a_concentration: 0.39330758068481686
            a_loc: 0.8371888967872619
            a_scale: 4.432725319432478
            a_bulge_disk_ratio: 2.0
    decoder:
        _target_: bliss.simulator.decoder.ImageDecoder
        n_bands: 1
        tile_slen: 4
        ptile_slen: 52
        border_padding: 24
        psf_slen: 25
        sdss_bands:
            - 2
        psf_params_file: ${paths.data}/sdss/94/1/12/psField-000094-1-0012.fits
        galaxy_model:
            _target_: bliss.simulator.galsim_galaxies.SingleGalsimGalaxyDecoder
            n_bands: 1
            slen: 53
            pixel_scale: 0.396
            psf_params_file: ${simulator.decoder.psf_params_file}
            psf_slen: 25
            sdss_bands:
                - 2
    background:
        _target_: bliss.simulator.background.SimulatedSDSSBackground
        sdss_dir: ${paths.sdss}
        run: 94
        camcol: 1
        field: 12
        bands:
            - 2
    n_tiles_h: 8
    n_tiles_w: 8
    n_batches: 128
    batch_size: 64
    generate_device: "cuda:0"
    fix_validation_set: true
    valid_n_batches: 10  #256
    num_workers: 0 # TODO: get 5 workers running and achieve ~6 training iterations/sec

encoder:
    _target_: bliss.encoder.Encoder
    n_bands: ${simulator.decoder.n_bands}
    tile_slen: ${simulator.decoder.tile_slen}
    tiles_to_crop: 6
    annotate_probs: True
    slack: 1.0
    optimizer_params:
        lr: 1e-3
    scheduler_params:
        milestones: [32]
        gamma: 0.1
    architecture:
        # this architecture is based on yolov5l.yaml, see
        # https://github.com/ultralytics/yolov5/blob/master/models/yolov5l.yaml
        depth_multiple: 1.0  # model depth multiple
        width_multiple: 1.0  # layer channel multiple
        anchors:
            - [4, 4]  # P3/8
        backbone: [
            # [from, number, module, args]
            [-1, 1, Conv, [64, 5, 1]],
            [-1, 3, Conv, [64, 1, 1]],
            [-1, 1, Conv, [128, 3, 2]],
            [-1, 1, Conv, [128, 3, 1]],
            [-1, 1, Conv, [256, 3, 2]],
            [-1, 6, C3, [256]],
            [-1, 1, Conv, [512, 3, 2]],
            [-1, 9, C3, [512]],
            [-1, 1, Conv, [1024, 3, 2]],
            [-1, 3, C3, [1024]],
            [-1, 1, SPPF, [1024, 5]],
        ]
        head: [
            [-1, 1, Conv, [512, 1, 1]],
            [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            [[-1, 6], 1, Concat, [1]],
            [-1, 3, C3, [512, false]],
            [-1, 1, Conv, [256, 1, 1]],
            [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            [[-1, 4, 5], 1, Concat, [1]],
            [-1, 3, C3, [256, false]],
            [[17], 1, Detect, [nc, anchors]],
        ]
    # TODO: implement pretraining and try pretraining with crowded fields
    # pretrained_weights: ${paths.project}/models/${training.name}.pt


training:
    name: "sdss_encoder"
    version: null
    n_epochs: 1000
    experiment: default
    save_top_k: 1
    trainer:
        _target_: pytorch_lightning.Trainer
        logger: True
        enable_checkpointing: False
        profiler: null
        reload_dataloaders_every_n_epochs: 0
        max_epochs: ${training.n_epochs}
        min_epochs: ${training.n_epochs}
        accelerator: "gpu"
        devices: 1
        limit_train_batches: 1.0
        limit_val_batches: 1.0
        check_val_every_n_epoch: 10
        log_every_n_steps: 10  # correspond to n_batches
    testing:
        file: null
        batch_size: 32
        num_workers: 0  # why not increase this?
    weight_save_path: ${paths.project}/models/${training.name}.pt
    seed: 42

inference:
    dataset: ${datasets.sdss_frame}
    encoder: ${encoder}
    weight_save_path: ${paths.project}/models/${training.name}.pt
