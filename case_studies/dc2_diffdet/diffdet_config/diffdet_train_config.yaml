---
defaults:
    - ../../../bliss/conf@_here_: base_config
    - _self_
    - override hydra/job_logging: stdout

mode: train

surveys:
  dc2:
    batch_size: 128

my_metrics:
  detection_performance:
    _target_: case_studies.dc2_diffdet.utils.metrics.DetectionPerformance

my_image_normalizers:
  null_noramlize:
      _target_: bliss.encoder.image_normalizer.NullNormalizer

encoder:
    _target_: case_studies.dc2_diffdet.utils.encoder.SparseRCNNDiffusionEncoder
    survey_bands: [u, g, r, i, z, y]
    reference_band: 2
    tile_slen: ${surveys.dc2.tile_slen}
    ddim_steps: 5
    ddim_beta_schedule: linear
    feature_backbone_path: /home/pduan/bliss_output/DC2_diffdet_pretrain_exp/exp_backbone/checkpoints/encoder_18.ckpt
    optimizer_params:
        lr: 1e-3
        amsgrad: true  # to make adam more stable
    scheduler_params:
        milestones: [30]
        gamma: 0.1
    image_normalizers: ${my_image_normalizers}
    image_size: [80, 80]
    matcher:
        _target_: bliss.encoder.metrics.CatalogMatcher
        dist_slack: 1.0
        mag_slack: null
        mag_band: 2  # SDSS r-band
    mode_metrics:
        _target_: torchmetrics.MetricCollection
        _convert_: partial
        compute_groups: false
        metrics: ${my_metrics}

train:
    trainer:
        logger:
            name: DC2_diffdet_exp
            version: exp_02-09-1  # change it before running the code
        devices: [7]  # change it before running the code
        use_distributed_sampler: false  # disable this because we use the self-defined distributed sampler
        precision: 32-true
        max_epochs: 50
        # detect_anomaly: false
        # gradient_clip_val: 0.0
    data_source: ${surveys.dc2}
    pretrained_weights: null
    seed: 7272
    callbacks:
        early_stopping:
            _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
            monitor: val/_loss
            mode: min
            patience: 25