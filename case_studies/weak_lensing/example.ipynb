{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3c3ec4",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565991a",
   "metadata": {},
   "source": [
    "The goal of this notebook is to illustrate the full BLISS pipeline, briefly. We will identify several of the main data objects used along the way, but avoid mathematical intricacies and details of the training objective. Briefly, we'll have\n",
    "\n",
    "1. Generation of synthetic data\n",
    "2. Training of the encoder network\n",
    "3. *(not currently in this notebook)* Validation/evaluation of the encoder network on held-out data (either simulated or real)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e81bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb35f7",
   "metadata": {},
   "source": [
    "#### 1. Generation of synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388caa8",
   "metadata": {},
   "source": [
    "This would typically be done using something like ```bliss mode=generate``` from the command line. This calls the ```generate``` function within ```bliss/main.py```, using the default ```DictConfig``` object specified by the various ```.yaml``` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48503ee",
   "metadata": {},
   "source": [
    "The code below loads the file ```m2_config.yaml``` from ```case_studies/dependent_tiling``` as a DictConfig for use by ```hydra```. You may have to change some absolute and relative paths to get this to load for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change twhit to your username\n",
    "os.chdir('/home/twhit/bliss')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04657ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bliss.encoder.variational_dist import VariationalDistSpec, VariationalDist\n",
    "from bliss.encoder.unconstrained_dists import UnconstrainedNormal\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from bliss.catalog import TileCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change twhit to your username\n",
    "environ[\"BLISS_HOME\"] = \"/home/twhit/bliss\"\n",
    "with initialize(config_path=\"../../case_studies/dependent_tiling\", version_base=None):\n",
    "    cfg = compose(\"m2_config\", overrides={\"surveys.sdss.load_image_data=true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031aed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(cfg, resolve=False, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09b90e",
   "metadata": {},
   "source": [
    "You can browse the above printouts to get a feel for how the config is structured. Our project will eventually add some configurables and we'll have our own config similar to the above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ff86b",
   "metadata": {},
   "source": [
    "The ```generate``` function from ```bliss/main.py``` takes arguments as\n",
    "\n",
    "```\n",
    "def generate(gen_cfg: DictConfig):\n",
    "   ...\n",
    "\n",
    "```\n",
    "\n",
    "and so we can plug in a given ```DictConfig``` like the above to generate data. The true ```generate``` function is much more complex than what is given below, and helps cache previously simulated data to save time, etc. Our altered function below is used only for illustration purposes, where we'll generate a single batch of simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f757dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gen_cfg: DictConfig):\n",
    "    simulated_dataset = instantiate(gen_cfg.simulator, num_workers=0)\n",
    "\n",
    "    for _ in range(1):\n",
    "        batch = simulated_dataset.get_batch()\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8875eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell will take a few minutes\n",
    "simulated_batch_of_data = generate(cfg.generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49107fd",
   "metadata": {},
   "source": [
    "Note that the 'full' configurable has three main subconfigs: `cfg.generate, cfg.train, cfg.predict`. This just helps keep things more organized. We only passed `cfg.generate` to the ```generate``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_batch_of_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0fa5e",
   "metadata": {},
   "source": [
    "The simulated batch is a ```dict``` object. These can be stored to disk in some fashion that we won't worry about right now (see ```generate``` function in ```bliss/main.py```), as we'll just work with this single batch. Let's examine some of the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80febdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = simulated_batch_of_data['tile_catalog']\n",
    "tc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['locs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['n_sources'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7343d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['source_type'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['galaxy_fluxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27784fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['galaxy_params'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910fbe5",
   "metadata": {},
   "source": [
    "The first three dimensions of all these objects are 32 x 56 x 56. These numbers represent the following:\n",
    "- 32 = number of synthetic (simulated) images\n",
    "- 56 = number of tiles lengthwise\n",
    "- 56 = number of tiles widthwise\n",
    "\n",
    "BLISS operates by dividing a given images into *tiles* of a certain size. This can be thought of as parsing the image into bite-size pieces. The number of tiles and number of images in simulated batches are controllable from the config object, e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1064cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.prior.batch_size, cfg.prior.n_tiles_h, cfg.prior.n_tiles_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905ea62",
   "metadata": {},
   "source": [
    "BLISS constrains the number of objects per tile to be between 0 and 5. Tiles are small enough (in terms of pixel size) to make this reasonable. Hence if we examine ```tc['n_sources']``` we see that this tensor specifies the number of sources for each tile in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['n_sources'][0] #56 x 56 tensor telling us the number of sources in each tile for the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6625598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc['n_sources'].max(), tc['n_sources'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a195f6",
   "metadata": {},
   "source": [
    "Each object (or source) can be one of several different types (although I think it's generally either a star or a galaxy). There are at most 5 sources per tile. Hence ```tc['source_type']``` has a per-tile shape of 5 x 1. For each tile, this tells us the type of each source. If there are fewer than 5 sources, the extra information is discarded or masked somehow. This number 5 that is floating around all the other dimensions arises similarly from the fact that we have at most 5 sources per tile. We can gather from the shapes above that ```locs``` contains a 2d coordinate for each source; ```galaxy_fluxes``` contains 5 parameters for each source; ```galaxy_params``` contains 6 parameters for each source, etc. \n",
    "\n",
    "***All of the these are latent random variables $z$ that are used to generate the image. Given a tile catalog like the above, we have all the information necessary to generate synthetic images $x$. The inference problem is then to take an image $x$ and construct a distribution on all of these quantities $z$. In other words, given an image, we divide it into tiles and for each tile we aim to recover the number of sources, the type of each source, the locations of each, the fluxes for each, etc.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8bb321",
   "metadata": {},
   "source": [
    "Let's examine a sample synthetic image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = simulated_batch_of_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea11845",
   "metadata": {},
   "source": [
    "As expected, there are 32 images. The dimension 5 does not correspond to the number of sources per tile, but rather to the *photometric band* of each image, corresponding to u,g,r,i,z for SDSS data. Read more here: https://www.sdss4.org/instruments/camera/#Filters\n",
    "\n",
    "Again, these are specified in the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.prior.survey_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798b6ac",
   "metadata": {},
   "source": [
    "We gather that each image for each band is 112 x 112. Recalling that we have 56 x 56 tiles, this implies that each tile is 2x2. Again, this is specified in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90191d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.prior.tile_slen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f794ed7",
   "metadata": {},
   "source": [
    "Let's examine some simulated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3872beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[0][0]) #u-band for first of 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[0][1]) #g-band for first of 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3120342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[0][2]) #r-band for first of 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[0][3]) #i-band for first of 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims[0][4]) #z-band for first of 32 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2f325",
   "metadata": {},
   "source": [
    "We notice that some objects appear not to show up in some bands, or appear more faint in some bands than others. This is normal: objects emit light at different wavelengths, and so in some wavelength ranges (e.g. a particular band) an object may not appear to be visible at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7840f63",
   "metadata": {},
   "source": [
    "We won't worry about the other keys of `simulated_batch_of_data` for now (`background`, `deconvolution`, and `psf_params`). We can explore these later as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff840b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207c586",
   "metadata": {},
   "source": [
    "#### 2. Training the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448575d",
   "metadata": {},
   "source": [
    "The encoder $q_{\\phi}(z \\mid x)$ learns a *variational distribution* on $z$ conditional on an observed $x$. We use machine learning and amortization to automate this process: Given any $x$, we get a distribution on $z$ by passing $x$ through a neural network (whose parameters are $\\phi$). For us, $z$ is a complicated object consisting of all the parameters in the tile catalog above (or more). \n",
    "\n",
    "The training objective is given by\n",
    "\n",
    "$$\n",
    "\\max_\\phi \\thinspace \\Bigl[\\mathbb{E}_{p(z,x)} \\log q_\\phi(z \\mid x)\\Bigr].\n",
    "$$\n",
    "\n",
    "In words, we want to find the neural network parameters $\\phi$ that maximize the variational density across all $z,x$ from the generative model $p(z,x)$. For us, the generative model is given by \n",
    "\n",
    "$$\n",
    "p(z,x) = p(z) p(x \\mid x)\n",
    "$$\n",
    "\n",
    "where $p(z)$ is the prior, and $p(x \\mid x)$ generates images from the prior. We won't focus much at all on $p(z \\mid x)$. Instead, we'll focus on the prior, and we will alter the tile catalog $z$ (e.g., to include new parameters that are specific to weak lensing, such as shear and convergence). The information for the prior is again given by the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(cfg.prior, resolve=False, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.prior.galaxy_a_bd_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a77410",
   "metadata": {},
   "source": [
    "These numbers can be considered as hyperparameters that define the prior. We don't actually need to infer these numbers precisely. Rather, for a tile catalog $z$ sampled from the prior $p(z)$, we want to infer $z$ given its corresponding image $x$.\n",
    "\n",
    "Training is performed using the ```train``` function from ```bliss/main.py```. As above, we pass `cfg.train` to the `train` function, which is reproduced below (with some lines omitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74eb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_cfg: DictConfig):\n",
    "    # setup seed\n",
    "    pl.seed_everything(train_cfg.seed)\n",
    "\n",
    "    # setup dataset, encoder, and trainer\n",
    "    dataset = instantiate(train_cfg.data_source)\n",
    "    encoder = instantiate(train_cfg.encoder)\n",
    "    trainer = instantiate(train_cfg.trainer)\n",
    "\n",
    "    # train!\n",
    "    trainer.fit(encoder, datamodule=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdb56d",
   "metadata": {},
   "source": [
    "Let's examine the three main objects that the training procedure evidently uses: a dataset, an encoder, and a trainer. Again, these are all specified by the config, and instatiated in the train function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train.data_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d3795",
   "metadata": {},
   "source": [
    "We see that the data-source is a cached dataset. In other words, lots of saved $z,x$ pairs from the generative model $p(z,x)$ that have been written to disk previously. These are used to fit the objective function approximately — i.e.,\n",
    "\n",
    "$$\n",
    "\\max_\\phi \\frac{1}{n} \\sum_{i=1}^n \\log q_\\phi(z_i \\mid x_i),\n",
    "$$\n",
    "\n",
    "where $n$ is the number of cached $z,x$ pairs that have been saved. For us, we only have one pair of $z,x$. We'll evidently need to convert these to type `CachedSimulatedDataset` to fit in with the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930eab00",
   "metadata": {},
   "source": [
    "All these configurables will instantiate an `Encoder` object. This object will perform the function described above, i.e. given an image $x$ it will give us a distribution on the tile catalog $z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train.trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74037e",
   "metadata": {},
   "source": [
    "The trainer evidently wraps up the training procedure with lots of information about logging, checkpoints, metrics, etc. We won't worry so much about this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d6908",
   "metadata": {},
   "source": [
    "Our simplified version of training for illustrative purposes will try to unwrap some of the abstraction above. Let's first instantiate the encoder so we can examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79121201",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = instantiate(cfg.train.encoder)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13482ce9",
   "metadata": {},
   "source": [
    "If you're familiar with PyTorch, you'll recognize that the encoder is essentially a huge neural network as described above, with some fancy preprocessing, normalization, metrics, etc. Let's examine some of the `Encoder` object's methods, which are reproduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70982c2d",
   "metadata": {},
   "source": [
    "```\n",
    "def _single_detection_nll(self, target_cat, pred):\n",
    "    marginal_loss = pred[\"marginal\"].compute_nll(target_cat)\n",
    "\n",
    "    if not self.use_checkerboard:\n",
    "        return marginal_loss\n",
    "\n",
    "    white_loss = pred[\"white\"].compute_nll(target_cat)\n",
    "    white_loss_mask = 1 - pred[\"white_history_mask\"]\n",
    "    white_loss *= white_loss_mask\n",
    "\n",
    "    black_loss = pred[\"black\"].compute_nll(target_cat)\n",
    "    black_loss_mask = pred[\"white_history_mask\"]\n",
    "    black_loss *= black_loss_mask\n",
    "\n",
    "    # we divide by two because we score two predictions for each tile\n",
    "    return (marginal_loss + white_loss + black_loss) / 2\n",
    "\n",
    "def _double_detection_nll(self, target_cat1, target_cat, pred):\n",
    "    target_cat2 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=1)\n",
    "\n",
    "    nll_marginal_z1 = self._single_detection_nll(target_cat1, pred)\n",
    "    nll_cond_z2 = pred[\"second\"].compute_nll(target_cat2)\n",
    "    nll_marginal_z2 = self._single_detection_nll(target_cat2, pred)\n",
    "    nll_cond_z1 = pred[\"second\"].compute_nll(target_cat1)\n",
    "\n",
    "    none_mask = target_cat.n_sources == 0\n",
    "    loss0 = nll_marginal_z1 * none_mask\n",
    "\n",
    "    one_mask = target_cat.n_sources == 1\n",
    "    loss1 = (nll_marginal_z1 + nll_cond_z2) * one_mask\n",
    "\n",
    "    two_mask = target_cat.n_sources >= 2\n",
    "    loss2a = nll_marginal_z1 + nll_cond_z2\n",
    "    loss2b = nll_marginal_z2 + nll_cond_z1\n",
    "    lse_stack = torch.stack([loss2a, loss2b], dim=-1)\n",
    "    loss2_unmasked = -torch.logsumexp(-lse_stack, dim=-1)\n",
    "    loss2 = loss2_unmasked * two_mask\n",
    "\n",
    "    return loss0 + loss1 + loss2\n",
    "```\n",
    "\n",
    "Here, NLL stands for negative log likelihood, and for us this corresponds to $-\\log q_\\phi(z \\mid x)$. We try to minimize this quantity, which is equivalent to maximizing $\\log q_\\phi(z \\mid x)$ (as we formulated above). Let's compute the NLL loss for the encoder. This should be poor because the encoder has been initialized only, not trained at all. The following code snippets are adapated from the `_compute_loss` function of the `Encoder` class in `bliss/encoder/encoder.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = simulated_batch_of_data #renaming to something shorter\n",
    "batch_size = batch[\"images\"].size(0)\n",
    "target_cat = TileCatalog(encoder.tile_slen, batch[\"tile_catalog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c693fb",
   "metadata": {},
   "source": [
    "The object `target_cat` is the \"target catalog\" of interest. It's of type TileCatalog that we create from the simulated batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a83498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out undetectable sources\n",
    "if encoder.min_flux_threshold > 0:\n",
    "    target_cat = target_cat.filter_tile_catalog_by_flux(min_flux=encoder.min_flux_threshold)\n",
    "    \n",
    "# make predictions/inferences\n",
    "target_cat1 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=0)\n",
    "truth_callback = lambda _: target_cat1\n",
    "pred = encoder.infer(batch, truth_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cbaa92",
   "metadata": {},
   "source": [
    "The above code performs some preprocessing on a per-tile basis to eliminate dim objects, etc. We don't need to understand all the details right now. The `infer` method of the `Encoder` object on the last line operates directly on the images of the batch i.e. `batch['images']`. It is a complex method that splits the image into tiles that are designated as white or black in a 'checkerboard' scheme --- this scheme helps with detection of objects at tile boundaries, which is a complex problem in its own right. We don't really need to understand the details of how all of this works for now. We do want to at least understand the form of the resulting object, which is stored in variable `pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35296f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed9627",
   "metadata": {},
   "source": [
    "These names will seem somewhat mysterious, and that's okay. We can learn more about them and how they are computed throughout the semester. The NLL functions reproduced above take a target catalog and the pred object above, and use these to compute the NLL loss. In other words, the quantity\n",
    "\n",
    "$$\n",
    "- \\log q_\\phi(z \\mid x)\n",
    "$$\n",
    "\n",
    "that we aim to compute is given by the following: firstly, $z$ is the `target_cat` of type `TileCatalog`. Recall that because we're generating synthetic data, the latent variable $z$ is not hidden, but known. The $x$ is given by the images from `batch['images']`, and these are operated on by the `infer` method of the encoder. The resulting computations yield the objects in `pred.keys()` above, which can be used to compute $-\\log q_\\phi(z \\mid x)$ for this particular data batch via the functions `_single_detection_nll` and `_double_detection_nll`. We don't need to go into detail as to how these are computed for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = encoder._double_detection_nll(target_cat1, target_cat, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ddc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3534e9",
   "metadata": {},
   "source": [
    "Loss is evidently computed on a per-image per-tile basis. We'll average across all of these because they all deserve equal weighting in our toy setup here. Now that we've illustrated how to compute the loss, let's wrap this all into a training loop to fit the encoder. We'll fit by optimizing the parameters directly rather than wrapping the procedure into a PyTorch Lightning routine as is done in the true BLISS code. \n",
    "\n",
    "This is extremely simplistic: we have a single batch of data $z,x$ that we generated above.  Nevertheless, in the training loop below, we still redefine/recompute `pred` and the target catalogs. In a true training procedure with many different batches of images, this would need to be done within the loop because we'll have a different batch of images every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8cdfc",
   "metadata": {},
   "source": [
    "***The cell below runs very slow because it's all CPU. True codebase is optimized to GPU. Naive use of GPU below will lead to out of memory error.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0df9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very small number of iterations, but it still takes 5-10 minutes\n",
    "# In practice, BLISS training will be a lot faster\n",
    "\n",
    "niter = 30\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(niter):\n",
    "    # Many of the lines below are redundant because we have a single batch\n",
    "    # so quantities don't change.\n",
    "    \n",
    "    target_cat = TileCatalog(encoder.tile_slen, batch[\"tile_catalog\"])\n",
    "    # filter out undetectable sources\n",
    "    if encoder.min_flux_threshold > 0:\n",
    "        target_cat = target_cat.filter_tile_catalog_by_flux(min_flux=encoder.min_flux_threshold)\n",
    "\n",
    "    # make predictions/inferences\n",
    "    target_cat1 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=0)\n",
    "    truth_callback = lambda _: target_cat1\n",
    "    pred = encoder.infer(batch, truth_callback)\n",
    "    \n",
    "    # Main gradient step code\n",
    "    optimizer.zero_grad()\n",
    "    loss = encoder._double_detection_nll(target_cat1, target_cat, pred).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Iteration {}: Loss {}'.format(i, loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b2447",
   "metadata": {},
   "source": [
    "The training procedure above is very rough: there's no learning rate tuning, scheduling, etc., and we don't run the fitting procedure all the way to convergence due to time constraints. Nevertheless, this is enough to get a feel for how fitting the encoder should generally go. In actuality, however, a lot of this will be abstracted away within PyTorch lightning procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17600a",
   "metadata": {},
   "source": [
    "We want to see how the encoder is doing. Recall that we just have a single data pair $z,x$ that we've been training with. It's a reasonable expectation that with enough training time our encoder should learn to output the correct $z$ given $x$ as an input. Let's check it out. We do this by using the `sample` method of the encoder. We'll simply take the posterior mode for now, i.e. the mode of the distribution $q_\\phi(z \\mid x)$, but we could generate more diverse samples from the actual distribution if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mode = encoder.sample(batch, use_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(posterior_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(posterior_mode).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8bae7",
   "metadata": {},
   "source": [
    "Let's compare the posterior model to the true target catalog $z$, which is the variable `target_cat` after wrapping up in this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mode.n_sources.shape, target_cat.n_sources.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12a901",
   "metadata": {},
   "source": [
    "BLISS usually omits border tiles. We'll want to do the same when we look at the target catalog for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mode.n_sources.shape, target_cat.n_sources[:,1:-1,1:-1, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.-(posterior_mode.n_sources != target_cat.n_sources[:,1:-1,1:-1, ...]).sum()/(32*54*54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57514da",
   "metadata": {},
   "source": [
    "The line above tells us the proportion of tiles across the 32 images in our batch in which the posterior mode identifies the correct number of sources. This proportion would probably increase if we trained longer. Note that the variational distribution constrains us to have at most 2 sources per tile, so it's not surprising that some are wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019360c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.-(torch.abs(posterior_mode.n_sources - target_cat.n_sources[:,1:-1,1:-1, ...]) > 1).sum()/(32*54*54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d6568",
   "metadata": {},
   "source": [
    "The line above tells us the proportion of tiles in our 32 images in which the number of sources detected differs from the true number of sources by no more than 1. In other words, even when the number of sources is wrong, it's usually off by no more than 1, mistaking 3 sources for 2, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89967dd2",
   "metadata": {},
   "source": [
    "If you want, you can also check if the predicted locations, fluxes, etc. look approximately correct. We can do this together at a later date."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
