{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5270d44e",
   "metadata": {},
   "source": [
    "# Working with Variational Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7410ad2",
   "metadata": {},
   "source": [
    "We'll illustrate how to use the `VariationalDist` and `VariationalDistSpec` objects in tandem with the `Encoder` to perform sampling and compute the NLL. Let's load and simulate some data according to the `base_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1117d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('/home/declan/current/bliss')\n",
    "from bliss.encoder.variational_dist import VariationalDistSpec, VariationalDist\n",
    "from bliss.encoder.unconstrained_dists import UnconstrainedNormal\n",
    "import torch\n",
    "import numpy as np\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from bliss.catalog import TileCatalog\n",
    "\n",
    "environ[\"BLISS_HOME\"] = \"/home/declan/current/bliss\"\n",
    "with initialize(config_path=\"../../bliss/conf\", version_base=None):\n",
    "    cfg = compose(\"base_config\", overrides={\"surveys.sdss.load_image_data=true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf56d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = instantiate(cfg.simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfab1c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bliss.simulator.simulated_dataset.SimulatedDataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac56e6",
   "metadata": {},
   "source": [
    "#### This cell below will take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c862f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = simulator.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b56e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tile_catalog', 'images', 'background', 'deconvolution', 'psf_params'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fc75dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['locs', 'n_sources', 'source_type', 'galaxy_fluxes', 'galaxy_params', 'star_fluxes'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['tile_catalog'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef37e3",
   "metadata": {},
   "source": [
    "Let's instaniate the `Encoder` and run its primary method (for our purposes): the `infer` method that operates on simulated batches of data. More precisely, infer operates on the images of the batch (`test_batch['images']` in our naming so far). First, let's hard-code the so-called `target_cat` aka target catalog to be the ground truth $z$. We need this to construct the variable `truth_callback` below (we won't worry too much about the motivation behind this for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2708b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = instantiate(cfg.train.encoder)\n",
    "target_cat = TileCatalog(encoder.tile_slen, test_batch[\"tile_catalog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8df9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TileCatalog(64 x 20 x 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d69254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out undetectable sources\n",
    "if encoder.min_flux_threshold > 0:\n",
    "    target_cat = target_cat.filter_tile_catalog_by_flux(min_flux=encoder.min_flux_threshold)\n",
    "    \n",
    "# make predictions/inferences\n",
    "target_cat1 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=0)\n",
    "truth_callback = lambda _: target_cat1\n",
    "pred = encoder.infer(test_batch, truth_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e2467",
   "metadata": {},
   "source": [
    "Let's examine the outputs of the `infer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c1b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_features', 'marginal', 'history_cat', 'white_history_mask', 'white', 'black'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a2459",
   "metadata": {},
   "source": [
    "We'll be most concerned with the dict entry `marginal`. As the name suggests, this will contain all information necessary for constructing the marginal variational distributions. In general, if a variational family is mean-field, it factorizes as the product of the marginals\n",
    "\n",
    "$$\n",
    "q(z_1, z_2 \\mid x) = q(z_1 \\mid x) q(z_2 \\mid x)\n",
    "$$\n",
    "\n",
    "hence the naming. For each latent variable of interest, we just need to know the marginal variational distribution on it when using this particular form of mean-field variational family. BLISS uses this type of variational family for the most part, although there are some subtleties whereby adjacent tiles do interact with one another. Let's focus on the pure mean-field case for now. If we have a distribution on each latent variable of interest, we can compute\n",
    "\n",
    "$$\n",
    "\\log q(z_1, \\dots, z_K \\mid x) = \\sum_{i=1}^K \\log q(z_i \\mid x)\n",
    "$$\n",
    "\n",
    "by log properties. We shall see that this is essentially how the `VariationalDist` objects compute the NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7f152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bliss.encoder.variational_dist.VariationalDist"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred['marginal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1c8311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'on_prob': Categorical(probs: torch.Size([64, 20, 20, 2])),\n",
       " 'loc': TruncatedDiagonalMVN(Normal(loc: torch.Size([64, 20, 20, 2]), scale: torch.Size([64, 20, 20, 2]))),\n",
       " 'galaxy_prob': Categorical(probs: torch.Size([64, 20, 20, 2])),\n",
       " 'galsim_disk_frac': TransformedDistribution(),\n",
       " 'galsim_beta_radians': TransformedDistribution(),\n",
       " 'galsim_disk_q': TransformedDistribution(),\n",
       " 'galsim_a_d': LogNormal(),\n",
       " 'galsim_bulge_q': TransformedDistribution(),\n",
       " 'galsim_a_b': LogNormal(),\n",
       " 'star_flux_u': LogNormal(),\n",
       " 'star_flux_g': LogNormal(),\n",
       " 'star_flux_r': LogNormal(),\n",
       " 'star_flux_i': LogNormal(),\n",
       " 'star_flux_z': LogNormal(),\n",
       " 'galaxy_flux_u': LogNormal(),\n",
       " 'galaxy_flux_g': LogNormal(),\n",
       " 'galaxy_flux_r': LogNormal(),\n",
       " 'galaxy_flux_i': LogNormal(),\n",
       " 'galaxy_flux_z': LogNormal()}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['marginal'].factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1aaae",
   "metadata": {},
   "source": [
    "We see that `pred['marginal']` returns an object of type `VariationalDist`. This variational distribution is conditional on the $x$ defined by our particular simulated batch.\n",
    "\n",
    "The `factors` attribute contains each of the marginal factors. Some of the shapes are revealing. Clearly 64 x 20 x 20 corresponds to the number of images in a simulated batch and the tiles corresponding to the `base_config` we've used. These can be altered if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60bcdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.prior.batch_size, cfg.prior.n_tiles_h, cfg.prior.n_tiles_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7a24b",
   "metadata": {},
   "source": [
    "This informs some of the dimensions we see above. For example, `loc` should 2D ***per-source***, an (x,y) coordinate pair. Hence the 2 in the last dimension for `loc`. Evidently, many of these distributions are per-source.\n",
    "\n",
    "Recall that BLISS's variational distribution currently allows for at most 2 sources per tile. This does not seem to be accounted for in the above. The way BLISS works in reality is to first detect the brightest source in each tile; then, having accounted for this, try to find a second source. In this `base_config` we don't do this actually: the prior is constrained to have at most 1 source per-tile so it's not necessary. If one wanted to detect multiple sources per-tile, one could change by overwriting as below. To detect multiple sources, one needs to implement this two-stage process. This can be done by setting the configurable `encoder.double_detect` to be `True` (and potentially some other configurables).\n",
    "\n",
    "For now, we won't worry about this and will be satisfied with generating and detecting at most 1 source per tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1109f920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(cfg.prior.max_sources) # Should be =1\n",
    "print(cfg.encoder.double_detect) #Should be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50787a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would change these configurables to detect multiple sources via a two-layer detection stage in encoder.\n",
    "# cfg.prior.max_sources = 2\n",
    "# cfg.encoder.double_detect = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b8092",
   "metadata": {},
   "source": [
    "How does the `infer` method of `Encoder` produce a `VariationalDist` object? Through the `VariationalDistSpec` class. This \"variational distribution specificiation\" contains the information needed to construct the variational distribution. Here's a look at the class below. The most important attribute is `factor_specs` which specifies the factors for each latent variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efb1f7",
   "metadata": {},
   "source": [
    "```\n",
    "class VariationalDistSpec(torch.nn.Module):\n",
    "    def __init__(self, survey_bands, tile_slen):\n",
    "        super().__init__()\n",
    "\n",
    "        self.survey_bands = survey_bands\n",
    "        self.tile_slen = tile_slen\n",
    "\n",
    "        self.factor_specs = {\n",
    "            \"on_prob\": UnconstrainedBernoulli(),\n",
    "            \"loc\": UnconstrainedTDBN(),\n",
    "            \"galaxy_prob\": UnconstrainedBernoulli(),\n",
    "            # galsim parameters\n",
    "            \"galsim_disk_frac\": UnconstrainedLogitNormal(),\n",
    "            \"galsim_beta_radians\": UnconstrainedLogitNormal(high=torch.pi),\n",
    "            \"galsim_disk_q\": UnconstrainedLogitNormal(),\n",
    "            \"galsim_a_d\": UnconstrainedLogNormal(),\n",
    "            \"galsim_bulge_q\": UnconstrainedLogitNormal(),\n",
    "            \"galsim_a_b\": UnconstrainedLogNormal(),\n",
    "        }\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5558192",
   "metadata": {},
   "source": [
    "The `Encoder` is instantiated with a `VariationalDistSpec` attribute. The `infer` method then uses the `make_dist` method of `VariationalDistSpec` to create the `VariationalDist` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed8c720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalDistSpec()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vd_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334dcf7a",
   "metadata": {},
   "source": [
    "We will present just a little more detail on how `Encoder` produces the final variational distribution. Look inside `Encoder.infer` for more detail. \n",
    "\n",
    "For any given batch, the images are first normalized, then passed through a feature net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0909c1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 20, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_images = encoder.image_normalizer.get_input_tensor(test_batch)\n",
    "x_features = encoder.features_net(normalized_images)\n",
    "x_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99589189",
   "metadata": {},
   "source": [
    "It appears that each tile is represented by a vector of length 256. This tensor is then passed to `marginal_net`  which produces all necessary variational parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b21e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 20, 38])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat = encoder.marginal_net(x_features)\n",
    "x_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ba50e",
   "metadata": {},
   "source": [
    "The shape of `x_cat` is revealing. It tells us that per-tile, we have 38 variational parameters. Let's recover this number 38 from the variational distribution specification to check that everything matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5bf6aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dist.dim for _,dist in encoder.vd_spec.factor_specs.items() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9039a9",
   "metadata": {},
   "source": [
    "So this magical number 38 just matches the dimensions totaled by all the variational distributions. If we added more variational distributions, this number would have to change obviously (it would increase, to 40, 45, etc.\n",
    "\n",
    "Let's show how `VariationalDist` is created from `x_cat`. As said above, this is done using the `make_dist` method of `VariationalDistSpec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015adafe",
   "metadata": {},
   "source": [
    "```\n",
    "def make_dist(self, x_cat):\n",
    "    factors = self._parse_factors(x_cat)\n",
    "    return VariationalDist(factors, self.survey_bands, self.tile_slen)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271f3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = encoder.vd_spec.make_dist(x_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac3cf0",
   "metadata": {},
   "source": [
    "Once one has the `VariationalDist` object created, one can compute NLL and sample using the functions quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2619df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd.compute_nll(target_cat1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fdbb429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TileCatalog(64 x 20 x 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd.sample(use_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2029b2d",
   "metadata": {},
   "source": [
    "# Redshift-Focused Variational Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa5ca5",
   "metadata": {},
   "source": [
    "I've extended the `VariationalDistSpec` and `VariationalDist` classes to `RedshiftVariationalDistSpec` and `RedshiftVariationalDist`. The main differences between these is that they add a new variational distribution on redshift to the list of parameters above. The config `redshift.yaml` extends the `base_config.yaml` so similarly both generates and detects at most one source per tile. We can make it more complicated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea53e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalog import RedshiftTileCatalog\n",
    "from variational_dist import RedshiftVariationalDistSpec, RedshiftVariationalDist\n",
    "\n",
    "environ[\"BLISS_HOME\"] = \"/home/declan/current/bliss\"\n",
    "with initialize(config_path=\".\", version_base=None):\n",
    "    cfg = compose(\"redshift\", overrides={\"surveys.sdss.load_image_data=true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7eb385b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['locs', 'n_sources', 'source_type', 'galaxy_fluxes', 'galaxy_params', 'star_fluxes', 'redshifts'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator = instantiate(cfg.simulator)\n",
    "test_batch = simulator.get_batch()\n",
    "test_batch['tile_catalog'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2fb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = instantiate(cfg.train.encoder)\n",
    "target_cat = RedshiftTileCatalog(encoder.tile_slen, test_batch[\"tile_catalog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef9ad66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bliss.encoder.encoder.Encoder"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671239cc",
   "metadata": {},
   "source": [
    "We do not need to modify/extend the Encoder class at this point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67ca6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out undetectable sources\n",
    "if encoder.min_flux_threshold > 0:\n",
    "    target_cat = target_cat.filter_tile_catalog_by_flux(min_flux=encoder.min_flux_threshold)\n",
    "    \n",
    "# get brightest sources\n",
    "target_cat1 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9d1b8",
   "metadata": {},
   "source": [
    "Let's make an example `RedshiftVariationalDistSpec` object to illustrate the new `factor_spec` on redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83c10da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvds = RedshiftVariationalDistSpec(cfg.prior.survey_bands, cfg.prior.tile_slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "044da9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on_prob',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedBernoulli at 0x7efca310fc70>),\n",
       " ('loc',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedTDBN at 0x7efca310fca0>),\n",
       " ('galaxy_prob',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedBernoulli at 0x7efca310ec20>),\n",
       " ('galsim_disk_frac',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogitNormal at 0x7efca310fb80>),\n",
       " ('galsim_beta_radians',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogitNormal at 0x7efca310f280>),\n",
       " ('galsim_disk_q',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogitNormal at 0x7efca310eef0>),\n",
       " ('galsim_a_d',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310ee60>),\n",
       " ('galsim_bulge_q',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogitNormal at 0x7efca310f520>),\n",
       " ('galsim_a_b',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310f5e0>),\n",
       " ('star_flux_u',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310f130>),\n",
       " ('star_flux_g',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310f3a0>),\n",
       " ('star_flux_r',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310f160>),\n",
       " ('star_flux_i',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efca310e9e0>),\n",
       " ('star_flux_z',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554c910>),\n",
       " ('galaxy_flux_u',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554f940>),\n",
       " ('galaxy_flux_g',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554fbe0>),\n",
       " ('galaxy_flux_r',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554e890>),\n",
       " ('galaxy_flux_i',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554f700>),\n",
       " ('galaxy_flux_z',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedLogNormal at 0x7efb5554f0a0>),\n",
       " ('redshift',\n",
       "  <bliss.encoder.unconstrained_dists.UnconstrainedNormal at 0x7efca310f1f0>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rvds.factor_specs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286f20b",
   "metadata": {},
   "source": [
    "We see that `rvds.factor_specs` contains a new variational distribution unique for this project, a distribution redshift. It's a Gaussian distribution for now, but be made to be anything one desires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca6491",
   "metadata": {},
   "source": [
    "The `Encoder` object should have a `RedshiftVariationalDistSpec` as its `vd_spec` attribute now. This was done by changing the appropriate `_target_` in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "495c316b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedshiftVariationalDistSpec()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vd_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29876a4e",
   "metadata": {},
   "source": [
    "Using the output of `Encoder.infer` thus get an instance of `RedshiftVariationalDist` that we can use to compute the NLL and sample, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "225fbc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_callback = lambda _: target_cat1\n",
    "pred = encoder.infer(test_batch, truth_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fb5d8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedshiftVariationalDist()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['marginal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c1aacdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dist.dim for _,dist in encoder.vd_spec.factor_specs.items() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e995c8",
   "metadata": {},
   "source": [
    "There are now 40 variational parameters per source, up from 38 before. We added two: a location and scale parameter for the Gaussian distribution that describes redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6fcd1",
   "metadata": {},
   "source": [
    "Let's compute NLL and sample to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d19fda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvd = pred['marginal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8511d85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedshiftTileCatalog(64 x 20 x 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvd.sample(use_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05454135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 20])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvd.compute_nll(target_cat1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522acc0",
   "metadata": {},
   "source": [
    "### Example Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c1ebc",
   "metadata": {},
   "source": [
    "***This cell runs pretty slowly. All on CPU for illustration.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac582a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 31.29745101928711\n",
      "Iteration 10: Loss 5.976312160491943\n",
      "Iteration 20: Loss 5.461785316467285\n",
      "Iteration 30: Loss 4.9790143966674805\n",
      "Iteration 40: Loss 4.622425079345703\n",
      "Iteration 50: Loss 4.379827976226807\n",
      "Iteration 60: Loss 4.2173662185668945\n",
      "Iteration 70: Loss 4.101867198944092\n",
      "Iteration 80: Loss 4.002026557922363\n",
      "Iteration 90: Loss 3.91727352142334\n",
      "Iteration 100: Loss 3.8605949878692627\n",
      "Iteration 110: Loss 3.7549383640289307\n",
      "Iteration 120: Loss 3.747856855392456\n",
      "Iteration 130: Loss 3.656813383102417\n",
      "Iteration 140: Loss 3.6292967796325684\n",
      "Iteration 150: Loss 3.5320780277252197\n",
      "Iteration 160: Loss 3.470863103866577\n",
      "Iteration 170: Loss 3.44598388671875\n",
      "Iteration 180: Loss 3.3535513877868652\n",
      "Iteration 190: Loss 3.4474265575408936\n",
      "Iteration 200: Loss 3.3439383506774902\n",
      "Iteration 210: Loss 3.2493293285369873\n",
      "Iteration 220: Loss 3.5173134803771973\n",
      "Iteration 230: Loss 3.2241787910461426\n",
      "Iteration 240: Loss 3.138590097427368\n",
      "Iteration 250: Loss 3.0926873683929443\n",
      "Iteration 260: Loss 3.7236886024475098\n",
      "Iteration 270: Loss 3.541830062866211\n",
      "Iteration 280: Loss 3.4552664756774902\n",
      "Iteration 290: Loss 3.385012626647949\n"
     ]
    }
   ],
   "source": [
    "niter = 300\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "for i in range(niter):\n",
    "    # Many of the lines below are redundant because we have a single batch\n",
    "    # so quantities don't change.\n",
    "    \n",
    "    target_cat = RedshiftTileCatalog(encoder.tile_slen, test_batch[\"tile_catalog\"])\n",
    "    # filter out undetectable sources\n",
    "    if encoder.min_flux_threshold > 0:\n",
    "        target_cat = target_cat.filter_tile_catalog_by_flux(min_flux=encoder.min_flux_threshold)\n",
    "\n",
    "    # make predictions/inferences\n",
    "    target_cat1 = target_cat.get_brightest_sources_per_tile(band=2, exclude_num=0)\n",
    "    truth_callback = lambda _: target_cat1\n",
    "    pred = encoder.infer(test_batch, truth_callback)\n",
    "    rvd = pred['marginal']\n",
    "    \n",
    "    # Main gradient step code\n",
    "    optimizer.zero_grad()\n",
    "    loss = rvd.compute_nll(target_cat1).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteration {}: Loss {}'.format(i, loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a5035",
   "metadata": {},
   "source": [
    "Let's check how we're doing on the redshift variational distributions. Recall the prior on redshift is extremely concentrated as this is a toy case for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4181d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99, 1.01)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.prior.redshift_min, cfg.prior.redshift_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e638457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9924, 0.9927, 0.9909,  ..., 0.9837, 0.9853, 0.9877],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = rvd.factors\n",
    "q[\"redshift\"].loc.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06250349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0416, 0.0426, 0.0425,  ..., 0.0421, 0.0418, 0.0419],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[\"redshift\"].scale.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222cc63",
   "metadata": {},
   "source": [
    "The locations are looking pretty good; the scales are still way too big, but at least the variational distributions are overdispersed. The variational distribution is misspecified: the prior is $\\textrm{Unif}(0.99, 1.01)$, and we have not modified the decoder $p(x \\mid z)$ at all. In other words, redshift has no impact on the data right now. Accordingly, the posterior should be equal to the prior.\n",
    "\n",
    "Of course we don't achieve that, because the variational distribution is constained to be Gaussian. We'd hope with more trainin we could get even more highly concentrated Gaussians approximately in the interval $[0.99,1.01]$ but the above suffices for now as a sanity check."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
