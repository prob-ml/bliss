{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3578c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from hydra.utils import instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c107e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from bliss.main import predict\n",
    "\n",
    "environ[\"BLISS_HOME\"] = str(\"/home/declan/current/bliss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d390d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/declan/current/bliss'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"BLISS_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9e36fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/declan/current/bliss/case_studies/redshift_estimation/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8938d5",
   "metadata": {},
   "source": [
    "I have stored the two checkpoints in the following locations:\n",
    "   1.  `/data/scratch/declan/sdss_encoder_ckpt.ckpt` for the SDSS-like galaxies\n",
    "   2.  `/data/scratch/declan/dc2_encoder_ckpt.ckpt` for the DC2-like galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7587ad",
   "metadata": {},
   "source": [
    "Recall that the corresponding directories containing the data used to train these two encoder are in\n",
    "   1. `/data/scratch/declan/sdss_like_galaxies` for the SDSS-like galaxies\n",
    "   2. `/data/scratch/declan/dc2_like_galaxies` for the DC2-like galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253b755",
   "metadata": {},
   "source": [
    "Below, I will load the DC2 checkpoint for example. You should change both fields below to the corresponding ones for SDSS-like data. Note I give a checkpoint path and the location of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c941b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../\", version_base=None):\n",
    "    cfg = compose(\"redshift\", {\n",
    "        \"predict.weight_save_path=/data/scratch/declan/dc2_encoder_ckpt.ckpt\",\n",
    "        \"cached_simulator.cached_data_path=/data/scratch/declan/dc2_like_galaxies\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a815635",
   "metadata": {},
   "source": [
    "This cell will take a while because it loads all of the training data (100 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741a689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(cfg.train.seed)\n",
    "\n",
    "# setup dataset and encoder\n",
    "dataset = instantiate(cfg.train.data_source)\n",
    "encoder = instantiate(cfg.train.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df3ba0",
   "metadata": {},
   "source": [
    "We aren't technically in `predict` mode so we need to manually load the checkpoint to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb080edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/data/scratch/declan/dc2_encoder_ckpt.ckpt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "encoder.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf7e55",
   "metadata": {},
   "source": [
    "Your tasks are as follows:\n",
    "- From the dataset, plot representative example images (you only need a few, like 2-3. You can pick the best ones to include in the write-up).\n",
    "- Feed these images through the encoder to get predictions for all quantities. Plot predictions of location (e.g. with an \"x\" market) overlaid on the top of the example images. If stuff is working well, these should be right in the center of the galaxies approximately. Can you think of any clever ways to visualize redshift predictions for one example image?\n",
    "- Create a scatterplot of predicted redshift vs. true redshift for all data in the training set. You can do an out-of-sample plot for validation data as well.\n",
    "- Compute metrics such as MSE and NLL averaged across the training dataset. You can also do an out-of-sample plot for validation data as well.\n",
    "\n",
    "\n",
    "Below, I work with an example observation just to provide a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15426e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataset.train_dataloader() # the data we trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eedd5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = dataset.val_dataloader() # didn't train on, but used to choose checkpoint I think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624e256",
   "metadata": {},
   "source": [
    "You will have to iterate through the whole dataloader of training and/or validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76e0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5534a7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'background', 'deconvolution', 'psf_params', 'tile_catalog'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eadca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 80, 80])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['images'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d4b63",
   "metadata": {},
   "source": [
    "Recall the tile catalog contains the \"true\" values used to generate the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92b11ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['locs', 'n_sources', 'source_type', 'galaxy_fluxes', 'galaxy_params', 'star_fluxes', 'redshifts'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['tile_catalog'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161e25e",
   "metadata": {},
   "source": [
    "Let's use the untrained encoder for prediction (it should perform very badly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5d44a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_cat = encoder.sample(observation, use_mode=True) # I'm using the mode to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81ebfc",
   "metadata": {},
   "source": [
    "The estimated catalog `est_cat` now contains the predicted values for each quantity. We can compre to the ground truth above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75206f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['locs', 'n_sources', 'star_fluxes', 'source_type', 'galaxy_params', 'galaxy_fluxes', 'redshifts'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_cat = est_cat.to_dict()\n",
    "est_cat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5db8cc",
   "metadata": {},
   "source": [
    "When `observation` is passed to `encoder`, the encoder ignores the ground truth `observation['tile_catalog']` to make the prediction obviously. But now we can compare the prediction to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3d20670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3300, 1.3700, 1.3200, 1.3300, 1.3500, 1.3100, 1.3500, 1.3100, 1.3400,\n",
       "         1.3600, 1.3600, 1.3700, 1.3200, 1.3200, 1.3500, 1.3800, 1.3800, 1.4900],\n",
       "        [1.3500, 1.3700, 1.3100, 1.3400, 1.3600, 1.3300, 1.4000, 1.3800, 1.3600,\n",
       "         1.3400, 1.3800, 1.3600, 1.3700, 1.3200, 1.3600, 1.4100, 1.4000, 1.4200],\n",
       "        [1.3500, 1.3500, 1.3200, 1.3300, 1.3300, 1.3300, 1.3800, 1.4000, 1.3600,\n",
       "         1.3300, 1.4000, 1.3400, 1.3500, 1.3400, 1.3500, 1.3900, 1.3500, 1.3300],\n",
       "        [1.3300, 1.3000, 1.4000, 1.3100, 1.3900, 1.3200, 1.3500, 1.3800, 1.3300,\n",
       "         1.3500, 1.4100, 1.2900, 1.4300, 1.3800, 1.3700, 1.3500, 1.4100, 1.3500],\n",
       "        [1.3500, 1.3500, 1.2900, 1.3500, 1.3500, 1.3300, 1.3400, 1.3400, 1.4200,\n",
       "         1.3800, 1.3800, 1.3400, 1.4000, 1.4200, 1.3300, 1.3300, 1.3700, 1.3500],\n",
       "        [1.3600, 1.2900, 1.3300, 1.3300, 1.3200, 1.3300, 1.3300, 1.3400, 1.3400,\n",
       "         1.3400, 1.3300, 1.3100, 1.3300, 1.3200, 1.3400, 1.3500, 1.3500, 1.3300],\n",
       "        [1.3600, 1.2800, 1.3700, 1.3300, 1.3400, 1.3600, 1.3300, 1.3600, 1.3500,\n",
       "         1.3600, 1.3300, 1.3300, 1.3500, 1.3400, 1.3500, 1.3000, 1.3000, 1.3400],\n",
       "        [1.3500, 1.3000, 1.3600, 1.3600, 1.3700, 1.3400, 1.3300, 1.3400, 1.3300,\n",
       "         1.3200, 1.3500, 1.3500, 1.3700, 1.3400, 1.3100, 1.3200, 1.2600, 1.3100],\n",
       "        [1.3100, 1.3000, 1.3500, 1.3900, 1.3900, 1.3300, 1.3700, 1.3500, 1.4000,\n",
       "         1.3400, 1.3700, 1.3500, 1.4000, 1.3100, 1.3300, 1.3500, 1.3000, 1.3500],\n",
       "        [1.3700, 1.3300, 1.3100, 1.3100, 1.3400, 1.3700, 1.3300, 1.3100, 1.4200,\n",
       "         1.3800, 1.3500, 1.3400, 1.4000, 1.3900, 1.3400, 1.3400, 1.3100, 1.3300],\n",
       "        [1.3700, 1.3000, 1.2900, 1.3200, 1.3300, 1.3100, 1.3600, 1.3700, 1.3600,\n",
       "         1.3400, 1.3400, 1.3100, 1.3400, 1.3600, 1.4200, 1.3500, 1.3500, 1.3100],\n",
       "        [1.3300, 1.3800, 1.3400, 1.3300, 1.3500, 1.3200, 1.3300, 1.3100, 1.3500,\n",
       "         1.3600, 1.3500, 1.3700, 1.4100, 1.3200, 1.3800, 1.3300, 1.3300, 1.3400],\n",
       "        [1.3300, 1.3400, 1.3300, 1.3900, 1.3600, 1.3200, 1.3100, 1.3800, 1.3200,\n",
       "         1.4700, 1.3300, 1.3100, 1.4000, 1.3100, 1.3300, 1.3300, 1.3000, 1.3700],\n",
       "        [1.3300, 1.3200, 1.3400, 1.3900, 1.3300, 1.3300, 1.3300, 1.3300, 1.3400,\n",
       "         1.3500, 1.3200, 1.3300, 1.3300, 1.3300, 1.3400, 1.2700, 1.3300, 1.4700],\n",
       "        [1.3900, 1.3500, 1.3400, 1.3200, 1.3200, 1.3400, 1.3900, 1.3100, 1.3300,\n",
       "         1.3400, 1.3400, 1.3100, 1.3800, 1.3700, 1.3300, 1.3600, 1.3400, 1.3400],\n",
       "        [1.4300, 1.3600, 1.3300, 1.3500, 1.3400, 1.3900, 1.3200, 1.3200, 1.3100,\n",
       "         1.3700, 1.3600, 1.3700, 1.3600, 1.3400, 1.3200, 1.3400, 1.3600, 1.3200],\n",
       "        [1.3700, 1.3300, 1.3300, 1.3500, 1.3100, 1.3600, 1.3100, 1.3500, 1.3600,\n",
       "         1.3700, 1.3900, 1.2700, 1.3800, 1.3100, 1.3500, 1.3400, 1.3300, 1.3300],\n",
       "        [1.3700, 1.3200, 1.3400, 1.3400, 1.3600, 1.3200, 1.2800, 1.3300, 1.3300,\n",
       "         1.3700, 1.4500, 1.3700, 1.3700, 1.3000, 1.3300, 1.3300, 1.2900, 1.3300]],\n",
       "       grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(est_cat['redshifts'][0].reshape(18,18), decimals=2) # for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6d21dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['tile_catalog']['redshifts'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70736e",
   "metadata": {},
   "source": [
    "The shapes are off, as we see that our estimated catalog is 18x18. This is because when training on the true images, the edge is filtered out. We can ignore it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58694b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_redshifts = observation['tile_catalog']['redshifts'][0][1:-1, 1:-1].reshape((18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3356723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4424, 1.2134, 2.8498, 1.9530, 0.5773, 0.5007, 0.9868, 2.4184, 1.1428,\n",
       "         0.4347, 0.5634, 1.6868, 0.4143, 1.6364, 2.8903, 0.5156, 1.9588, 0.4310],\n",
       "        [0.6704, 2.3828, 0.9407, 2.6686, 0.5478, 1.0735, 0.5573, 2.7453, 2.8398,\n",
       "         2.0776, 1.4965, 1.2019, 2.1638, 0.7854, 2.0735, 1.5511, 1.1001, 0.8776],\n",
       "        [0.1326, 1.4051, 2.2161, 1.8173, 1.0458, 2.9106, 2.1056, 1.1803, 0.1648,\n",
       "         1.1238, 1.2248, 1.7003, 1.2856, 0.6593, 0.9938, 2.2289, 2.4001, 2.4243],\n",
       "        [0.8146, 1.7009, 0.6363, 0.9262, 1.6455, 1.0841, 0.5214, 1.3127, 0.4255,\n",
       "         0.6747, 2.0102, 2.2205, 1.0597, 1.7882, 0.8592, 2.5242, 0.7988, 0.8940],\n",
       "        [0.6264, 0.7303, 0.6737, 0.8170, 1.4574, 1.8295, 1.2547, 1.5315, 0.9660,\n",
       "         0.7526, 0.7736, 1.8675, 0.8731, 0.9475, 1.9461, 2.3829, 1.4235, 1.3090],\n",
       "        [1.2252, 0.9115, 1.0557, 0.9719, 0.5587, 0.5436, 0.5807, 0.3490, 0.3271,\n",
       "         2.5016, 1.2764, 0.5977, 1.4651, 2.8698, 0.4914, 1.2269, 1.1538, 0.4595],\n",
       "        [2.0515, 2.5911, 0.6990, 0.4672, 1.9981, 1.5460, 0.8947, 2.9265, 1.1094,\n",
       "         0.5227, 0.8124, 2.6757, 0.5131, 2.0876, 1.0579, 0.5281, 1.0678, 1.6351],\n",
       "        [1.4997, 0.4336, 0.6158, 1.7227, 1.6189, 0.8690, 2.8882, 0.9445, 0.4696,\n",
       "         2.6392, 1.4398, 0.7631, 2.1878, 1.8018, 0.8933, 0.4142, 1.7922, 1.4057],\n",
       "        [2.4442, 1.1725, 2.5615, 1.5456, 0.7091, 0.4417, 0.7532, 1.2216, 1.8405,\n",
       "         1.0114, 0.8838, 1.0009, 1.8682, 0.3149, 1.5213, 1.8545, 1.4063, 2.5931],\n",
       "        [2.3766, 0.5888, 0.8878, 1.9363, 1.1658, 0.5123, 0.6666, 0.7944, 0.7655,\n",
       "         1.9147, 2.5979, 2.5385, 1.4867, 1.8856, 1.0242, 1.4960, 2.5435, 2.1964],\n",
       "        [0.3977, 2.5574, 1.2993, 0.5706, 0.5476, 2.4449, 0.7271, 0.7291, 1.4158,\n",
       "         2.4398, 1.1905, 0.4516, 1.0111, 1.2499, 2.3482, 0.5771, 0.5883, 1.8748],\n",
       "        [1.3497, 0.8043, 2.1478, 1.4849, 0.7809, 1.4686, 1.4898, 1.8572, 0.7222,\n",
       "         1.8991, 0.9300, 0.5403, 0.3649, 1.0093, 1.6537, 1.0489, 0.7760, 0.6347],\n",
       "        [1.5134, 1.3194, 1.4499, 2.3015, 0.8850, 0.5468, 2.3873, 0.3483, 1.1100,\n",
       "         0.9315, 1.5860, 1.7341, 1.1995, 1.6980, 2.8948, 1.9815, 0.8707, 1.5194],\n",
       "        [0.6471, 1.5788, 1.6137, 1.2198, 2.1081, 1.5392, 0.3085, 1.0132, 1.8255,\n",
       "         0.6707, 1.3240, 1.4176, 0.5033, 0.7050, 1.5769, 1.8040, 1.3506, 2.8135],\n",
       "        [1.5184, 0.8569, 0.9931, 0.7718, 2.0936, 0.7825, 2.3132, 0.4706, 0.7630,\n",
       "         1.9527, 2.8320, 1.0988, 0.8000, 0.6191, 1.0008, 0.7738, 2.2045, 2.6840],\n",
       "        [0.5577, 1.8356, 1.4067, 0.5860, 0.4534, 1.5175, 0.8037, 1.1922, 1.4674,\n",
       "         0.7378, 1.4864, 0.1571, 0.5416, 1.8984, 0.6031, 2.1362, 1.3330, 1.9663],\n",
       "        [0.3851, 0.9298, 1.8121, 2.4736, 2.6229, 1.9146, 2.3726, 0.9810, 1.0286,\n",
       "         1.1337, 1.8299, 1.0356, 1.6818, 1.0884, 2.2380, 0.6510, 1.5849, 1.1161],\n",
       "        [2.6771, 1.4800, 1.6787, 1.0583, 0.3244, 1.2051, 0.6191, 0.8126, 0.6321,\n",
       "         2.1589, 1.2006, 0.3348, 2.9682, 1.7694, 1.2036, 2.7588, 0.8875, 2.7577]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fef5bc",
   "metadata": {},
   "source": [
    "At a glance, looks pretty poor. I suspect SDSS is better and something simply went wrong with DC2 (we'll investigate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad44ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
