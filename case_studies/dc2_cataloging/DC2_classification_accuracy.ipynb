{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC2 Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from bliss.surveys.dc2 import DC2, unsqueeze_tile_dict\n",
    "from pathlib import Path\n",
    "\n",
    "from bliss.catalog import FullCatalog\n",
    "\n",
    "from torchmetrics import MetricCollection\n",
    "import tqdm\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "from bliss.catalog import SourceType\n",
    "\n",
    "import GCRCatalogs\n",
    "\n",
    "environ[\"BLISS_HOME\"] = str(Path().resolve().parents[1])\n",
    "\n",
    "output_dir = Path(\"./DC2_classification_accuracy/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\".\", version_base=None):\n",
    "    notebook_cfg = compose(\"notebook_config\")\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2: DC2 = instantiate(notebook_cfg.surveys.dc2)\n",
    "test_sample = dc2.get_plotting_sample(0)\n",
    "cur_image_wcs = test_sample[\"wcs\"]\n",
    "cur_image_true_full_catalog = test_sample[\"full_catalog\"]\n",
    "cur_image_match_id = test_sample[\"match_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCRCatalogs.set_root_dir(\"/data/scratch/dc2_nfs/\")\n",
    "lsst_catalog_gcr = GCRCatalogs.load_catalog(\"desc_dc2_run2.2i_dr6_object_with_truth_match\")\n",
    "lsst_catalog_sub = lsst_catalog_gcr.get_quantities(\n",
    "    [\n",
    "        \"id_truth\",\n",
    "        \"objectId\",\n",
    "        \"ra\",\n",
    "        \"dec\",\n",
    "        \"truth_type\",\n",
    "        \"cModelFlux_u\",\n",
    "        \"cModelFluxErr_u\",\n",
    "        \"cModelFlux_g\",\n",
    "        \"cModelFluxErr_g\",\n",
    "        \"cModelFlux_r\",\n",
    "        \"cModelFluxErr_r\",\n",
    "        \"cModelFlux_i\",\n",
    "        \"cModelFluxErr_i\",\n",
    "        \"cModelFlux_z\",\n",
    "        \"cModelFluxErr_z\",\n",
    "        \"cModelFlux_y\",\n",
    "        \"cModelFluxErr_y\",\n",
    "    ]\n",
    ")\n",
    "lsst_catalog_df = pd.DataFrame(lsst_catalog_sub)\n",
    "lsst_catalog_tensors_dict = {\n",
    "    \"truth_type\": torch.tensor(lsst_catalog_df[\"truth_type\"].values).view(-1, 1),\n",
    "    \"flux\": torch.cat(\n",
    "        [\n",
    "            torch.tensor(flux.values).view(-1, 1)\n",
    "            for flux in [\n",
    "                lsst_catalog_df[\"cModelFlux_g\"],\n",
    "                lsst_catalog_df[\"cModelFlux_i\"],\n",
    "                lsst_catalog_df[\"cModelFlux_r\"],\n",
    "                lsst_catalog_df[\"cModelFlux_u\"],\n",
    "                lsst_catalog_df[\"cModelFlux_y\"],\n",
    "                lsst_catalog_df[\"cModelFlux_z\"],\n",
    "            ]\n",
    "        ],\n",
    "        dim=1,\n",
    "    ),\n",
    "    \"ra\": torch.tensor(lsst_catalog_df[\"ra\"].values),\n",
    "    \"dec\": torch.tensor(lsst_catalog_df[\"dec\"].values),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsst_params(\n",
    "    lsst_catalog_tensors_dict, cur_image_wcs, image_lim,\n",
    "):\n",
    "    lsst_ra = lsst_catalog_tensors_dict[\"ra\"]\n",
    "    lsst_dec = lsst_catalog_tensors_dict[\"dec\"]\n",
    "    lsst_pt, lsst_pr = cur_image_wcs.all_world2pix(lsst_ra, lsst_dec, 0)\n",
    "    lsst_pt = torch.from_numpy(lsst_pt)\n",
    "    lsst_pr = torch.from_numpy(lsst_pr)\n",
    "\n",
    "    lsst_plocs = torch.stack((lsst_pr, lsst_pt), dim=-1)\n",
    "    lsst_source_type = lsst_catalog_tensors_dict[\"truth_type\"]\n",
    "    lsst_flux = lsst_catalog_tensors_dict[\"flux\"]\n",
    "\n",
    "    x0_mask = (lsst_plocs[:, 0] > 0) & (lsst_plocs[:, 0] < image_lim)\n",
    "    x1_mask = (lsst_plocs[:, 1] > 0) & (lsst_plocs[:, 1] < image_lim)\n",
    "    lsst_x_mask = x0_mask * x1_mask\n",
    "    # filter r band\n",
    "    lsst_flux_mask = lsst_flux[:, 2] > 0\n",
    "    # filter supernova\n",
    "    lsst_source_mask = (lsst_source_type != 3).squeeze(1)\n",
    "    lsst_mask = lsst_x_mask * lsst_flux_mask * lsst_source_mask\n",
    "\n",
    "    lsst_plocs = lsst_plocs[lsst_mask, :]\n",
    "    lsst_source_type = torch.where(\n",
    "        lsst_source_type[lsst_mask] == 2, SourceType.STAR, SourceType.GALAXY\n",
    "    )\n",
    "    lsst_flux = lsst_flux[lsst_mask, :]\n",
    "\n",
    "    return lsst_plocs, lsst_source_type, lsst_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lim = test_sample[\"image\"].shape[1]\n",
    "r_band_min_flux = notebook_cfg.encoder.min_flux_for_metrics\n",
    "lsst_plocs, lsst_source_type, lsst_flux = get_lsst_params(\n",
    "    lsst_catalog_tensors_dict, cur_image_wcs, image_lim)\n",
    "flux_mask = lsst_flux[:, 2] > r_band_min_flux\n",
    "lsst_plocs = lsst_plocs[flux_mask, :]\n",
    "lsst_source_type = lsst_source_type[flux_mask]\n",
    "lsst_flux = lsst_flux[flux_mask, :]\n",
    "lsst_n_sources = torch.tensor([lsst_plocs.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_full_cat = FullCatalog(height=image_lim, width=image_lim, d={\n",
    "        \"plocs\": lsst_plocs.unsqueeze(0).to(device=device),\n",
    "        \"n_sources\": lsst_n_sources.to(device=device),\n",
    "        \"source_type\": lsst_source_type.unsqueeze(0).to(device=device),\n",
    "        \"galaxy_fluxes\": lsst_flux.unsqueeze(0).to(device=device),\n",
    "        \"star_fluxes\": lsst_flux.unsqueeze(0).clone().to(device=device),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this model path according to your training setting\n",
    "MODEL_PATH = \"../../output/DC2_experiments/DC2_psf_aug_asinh_06-06-1/checkpoints/best_encoder.ckpt\"\n",
    "bliss_encoder = instantiate(notebook_cfg.encoder).to(device=device)\n",
    "pretrained_weights = torch.load(MODEL_PATH, device)[\"state_dict\"]\n",
    "bliss_encoder.load_state_dict(pretrained_weights)\n",
    "bliss_encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 577, in _pydevd_bundle.pydevd_cython.PyDBFrame._handle_exception\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 312, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/home/pduan/bliss/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2070, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/home/pduan/bliss/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2106, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "batch = {\n",
    "    \"tile_catalog\": unsqueeze_tile_dict(test_sample[\"tile_catalog\"]),\n",
    "    \"images\": rearrange(test_sample[\"image\"], \"h w nw -> 1 h w nw\"),\n",
    "    \"background\": rearrange(test_sample[\"background\"], \"h w nw -> 1 h w nw\"),\n",
    "    \"psf_params\": rearrange(test_sample[\"psf_params\"], \"h w -> 1 h w\")\n",
    "}\n",
    "\n",
    "batch = move_data_to_device(batch, device=device)\n",
    "\n",
    "bliss_output_path = output_dir / \"bliss_output.pkl\"\n",
    "\n",
    "if not bliss_output_path.exists():\n",
    "    bliss_out_dict = bliss_encoder.predict_step(batch, None)\n",
    "\n",
    "    with open(bliss_output_path, \"wb\") as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(bliss_out_dict, outp, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(bliss_output_path, \"rb\") as inputp:\n",
    "        bliss_out_dict = pickle.load(inputp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bliss_full_cat: FullCatalog = bliss_out_dict[\"mode_cat\"].to_full_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = instantiate(notebook_cfg.encoder.matcher)\n",
    "bliss_metrics = instantiate(notebook_cfg.encoder.metrics)\n",
    "lsst_metrics = bliss_metrics.clone()\n",
    "bliss_metrics = MetricCollection({\n",
    "    \"source_type_accuracy\": bliss_metrics[\"source_type_accuracy\"],\n",
    "    \"source_type_accuracy_star\": bliss_metrics[\"source_type_accuracy_star\"],\n",
    "    \"source_type_accuracy_galaxy\": bliss_metrics[\"source_type_accuracy_galaxy\"],\n",
    "}).to(device=device)\n",
    "lsst_metrics = MetricCollection({\n",
    "    \"source_type_accuracy\": lsst_metrics[\"source_type_accuracy\"],\n",
    "    \"source_type_accuracy_star\": lsst_metrics[\"source_type_accuracy_star\"],\n",
    "    \"sourec_type_accuracy_galaxy\": lsst_metrics[\"source_type_accuracy_galaxy\"],\n",
    "}).to(device=device)\n",
    "\n",
    "bliss_results = {\n",
    "    \"classification_acc\": None,\n",
    "    \"classification_acc_star\": None,\n",
    "    \"classification_acc_galaxy\": None,\n",
    "}\n",
    "\n",
    "lsst_results = {\n",
    "    \"classification_acc\": None,\n",
    "    \"classification_acc_star\": None,\n",
    "    \"classification_acc_galaxy\": None,\n",
    "}\n",
    "\n",
    "classification_result_path = output_dir / \"classification_result.pkl\"\n",
    "if not classification_result_path.exists():\n",
    "    bliss_matching = matcher.match_catalogs(cur_image_true_full_catalog, bliss_full_cat)\n",
    "    bliss_metrics.update(cur_image_true_full_catalog, bliss_full_cat, bliss_matching)\n",
    "\n",
    "    lsst_matching = matcher.match_catalogs(cur_image_true_full_catalog, lsst_full_cat)\n",
    "    lsst_metrics.update(cur_image_true_full_catalog, lsst_full_cat, lsst_matching)\n",
    "\n",
    "    for k, v in bliss_metrics.items():\n",
    "        resutls = v.get_results_on_per_flux_bin()\n",
    "        for k_results, v_results in resutls.items():\n",
    "            bliss_results[k_results] = v_results.cpu()\n",
    "\n",
    "    for k, v in lsst_metrics.items():\n",
    "        resutls = v.get_results_on_per_flux_bin()\n",
    "        for k_results, v_results in resutls.items():\n",
    "            lsst_results[k_results] = v_results.cpu()\n",
    "\n",
    "        with open(classification_result_path, \"wb\") as classification_result_file:\n",
    "                pickle.dump({\n",
    "                    \"bliss_results\": bliss_results,\n",
    "                    \"lsst_results\": lsst_results,\n",
    "                }, classification_result_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(classification_result_path, \"rb\") as classification_result_file:\n",
    "          classification_result = pickle.load(classification_result_file)\n",
    "    bliss_results = classification_result[\"bliss_results\"]\n",
    "    lsst_results = classification_result[\"lsst_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(classification_acc_1,\n",
    "        classification_acc_2,\n",
    "        flux_bin_cutoffs,\n",
    "        source_type_name, \n",
    "        model_name_1, \n",
    "        model_name_2):\n",
    "    xlabels = (\n",
    "        [\"[100, \" + str(flux_bin_cutoffs[0]) + \"]\"]\n",
    "        + [f\"[{flux_bin_cutoffs[i]}, {flux_bin_cutoffs[i + 1]}]\" for i in range(len(flux_bin_cutoffs) - 1)]\n",
    "        + [\"> \" + str(flux_bin_cutoffs[-1])]\n",
    "    )\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(\n",
    "        1, 1, figsize=(10, 10), sharey=True\n",
    "    )\n",
    "\n",
    "    c1, c2 = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][0:2]\n",
    "    ax.plot(\n",
    "        range(len(xlabels)),\n",
    "        classification_acc_1.tolist(),\n",
    "        fmt=\"-o\",\n",
    "        color=c1,\n",
    "        label=f\"{model_name_1} Classification Acc ({source_type_name})\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(len(xlabels)),\n",
    "        classification_acc_2.tolist(),\n",
    "        fmt=\"-o\",\n",
    "        color=c2,\n",
    "        label=f\"{model_name_2} Classification Acc ({source_type_name})\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Flux\")\n",
    "    ax.set_xticks(range(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=45)\n",
    "    ax.legend()\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in bliss_metrics.items():\n",
    "    if k == \"source_type_accuracy\":\n",
    "        fig, ax = plot(bliss_results[\"classification_acc\"],\n",
    "                       lsst_results[\"classification_acc\"],\n",
    "                       flux_bin_cutoffs=v.flux_bin_cutoffs,\n",
    "                       source_type_name=v.source_type_name,\n",
    "                       model_name_1=\"BLISS\",\n",
    "                       model_name_2=\"LSST\")\n",
    "    elif k == \"source_type_accuracy_star\":\n",
    "        fig, ax = plot(bliss_results[\"classification_acc_star\"],\n",
    "                       lsst_results[\"classification_acc_star\"],\n",
    "                       flux_bin_cutoffs=v.flux_bin_cutoffs,\n",
    "                       source_type_name=v.source_type_name,\n",
    "                       model_name_1=\"BLISS\",\n",
    "                       model_name_2=\"LSST\")\n",
    "    elif k == \"source_type_accuracy_galaxy\":\n",
    "        fig, ax = plot(bliss_results[\"classification_acc_galaxy\"],\n",
    "                       lsst_results[\"classification_acc_galaxy\"],\n",
    "                       flux_bin_cutoffs=v.flux_bin_cutoffs,\n",
    "                       source_type_name=v.source_type_name,\n",
    "                       model_name_1=\"BLISS\",\n",
    "                       model_name_2=\"LSST\")\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
