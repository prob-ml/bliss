{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from bliss.surveys.dc2 import DC2DataModule\n",
    "import tqdm\n",
    "from bliss.catalog import TileCatalog\n",
    "from bliss.global_env import GlobalEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\".\", version_base=None):\n",
    "    notebook_cfg = compose(\"notebook_config\")\n",
    "notebook_cfg.surveys.dc2.train_transforms = []\n",
    "notebook_cfg.surveys.dc2.num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: cached data already exists at [/home/pduan/bliss_output/dc2_cached_data], we directly use it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc2: DC2DataModule = instantiate(notebook_cfg.surveys.dc2)\n",
    "dc2.prepare_data()\n",
    "dc2.setup(\"fit\")\n",
    "\n",
    "dc2_train_dataloader = dc2.train_dataloader()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3047/3047 [04:35<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate data time:  275.401s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GlobalEnv.seed_in_this_program = 0\n",
    "GlobalEnv.current_encoder_epoch = 0\n",
    "start_time = time.time()\n",
    "for batch in tqdm.tqdm(dc2_train_dataloader):\n",
    "    pass\n",
    "end_time = time.time()\n",
    "print(f\"iterate data time: {end_time - start_time: .3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# prevent pytorch_lightning warning for num_workers = 2 in dataloaders with IterableDataset\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*does not have many workers which may be a bottleneck.*\", UserWarning\n",
    ")\n",
    "# an IterableDataset isn't supposed to have a __len__ method\n",
    "warnings.filterwarnings(\"ignore\", \".*Total length of .* across ranks is zero.*\", UserWarning)\n",
    "\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_paths, shuffle=False, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.shuffle = shuffle\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        for file_path in files:\n",
    "            examples = torch.load(file_path)\n",
    "\n",
    "            # each training worker also shuffles the examples within each file\n",
    "            if self.shuffle:\n",
    "                random.shuffle(examples)\n",
    "\n",
    "            for ex in examples:\n",
    "                if self.transform is not None:\n",
    "                    ex = self.transform(ex)\n",
    "                yield ex\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        # shuffle files use for training each epoch\n",
    "        files = self.file_paths.copy()\n",
    "        if self.shuffle:\n",
    "            random.shuffle(files)\n",
    "\n",
    "        if worker_info is None:  # single-process data loading\n",
    "            files_subset = files\n",
    "        else:  # in a worker process\n",
    "            # split files evenly amongst workers\n",
    "            per_worker = int(math.ceil(len(files) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            files_subset = files[worker_id * per_worker : (worker_id + 1) * per_worker]\n",
    "\n",
    "        return iter(self.get_stream(files_subset))\n",
    "\n",
    "\n",
    "class CachedSimulatedDataset(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        splits: str,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "        cached_data_path: str,\n",
    "        train_transforms: List,\n",
    "        nontrain_transforms: List,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_transforms = train_transforms\n",
    "        self.nontrain_transforms = nontrain_transforms\n",
    "\n",
    "        file_names = [f for f in os.listdir(cached_data_path) if f.endswith(\".pt\")]\n",
    "        self.file_paths = [os.path.join(cached_data_path, f) for f in file_names]\n",
    "\n",
    "        # parse slices from percentages to indices\n",
    "        self.slices = self.parse_slices(splits, len(self.file_paths))\n",
    "\n",
    "    def _percent_to_idx(self, x, length):\n",
    "        \"\"\"Converts string in percent to an integer index.\"\"\"\n",
    "        return int(float(x.strip()) / 100 * length) if x.strip() else None\n",
    "\n",
    "    def parse_slices(self, splits: str, length: int):\n",
    "        slices = [slice(0, 0) for _ in range(3)]  # default to empty slice for each split\n",
    "        for i, data_split in enumerate(splits.split(\"/\")):\n",
    "            # map \"start_percent:stop_percent\" to slice(start_idx, stop_idx)\n",
    "            slices[i] = slice(*(self._percent_to_idx(val, length) for val in data_split.split(\":\")))\n",
    "        return slices\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        assert self.file_paths[self.slices[0]], \"No cached data found\"\n",
    "        transform = transforms.Compose(self.train_transforms)\n",
    "        my_dataset = MyIterableDataset(\n",
    "            self.file_paths[self.slices[0]], transform=transform, shuffle=True\n",
    "        )\n",
    "\n",
    "        return DataLoader(\n",
    "            my_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            worker_init_fn=random.seed,\n",
    "        )\n",
    "\n",
    "    def _get_nontrain_dataloader(self, file_paths_subset):\n",
    "        assert file_paths_subset, \"No cached data found\"\n",
    "        transform = transforms.Compose(self.nontrain_transforms)\n",
    "        my_dataset = MyIterableDataset(file_paths_subset, transform=transform)\n",
    "        return DataLoader(\n",
    "            my_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            worker_init_fn=random.seed,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_nontrain_dataloader(self.file_paths[self.slices[1]])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_nontrain_dataloader(self.file_paths[self.slices[2]])\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self._get_nontrain_dataloader(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_dataloader = CachedSimulatedDataset(\"0:80/80:90/90:100\", batch_size=64, num_workers=0, \n",
    "                                              cached_data_path=\"../../../bliss_output/dc2_cached_data\", \n",
    "                                              train_transforms=[], nontrain_transforms=[]).train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for batch in tqdm.tqdm(ori_train_dataloader):\n",
    "    pass\n",
    "end_time = time.time()\n",
    "print(f\"iterate data time: {end_time - start_time: .3f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
