{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_dataset(Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensors (list of Tensor): A list of PyTorch tensors representing your data.\n",
    "            labels (list, optional): A list of labels for the data.\n",
    "        \"\"\"\n",
    "        self.tensors = tensors\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensor = self.tensors[index]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            return tensor, self.labels[index]\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test(\n",
      "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout25): Dropout(p=0.25, inplace=False)\n",
      "  (dropout50): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=73728, out_features=512, bias=True)\n",
      "  (bn_fc): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)  # Changed from 3 to 4 channels\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout25 = nn.Dropout(0.25)\n",
    "        self.dropout50 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Calculate the correct input size\n",
    "        self.fc1_input_size = self._get_conv_output_size((4, 400, 400))\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 512)  # Adjusted dynamically\n",
    "        self.bn_fc = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout25(x)\n",
    "        \n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout25(x)\n",
    "        \n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout50(x)\n",
    "\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        x = self.dropout50(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        with torch.no_grad():\n",
    "            input = torch.rand(1, *input_size)\n",
    "            output = self._forward_features(input)\n",
    "            return output.data.view(1, -1).size(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "        x = self.dropout50(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = test()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(folder_paths=[\"data/\", \"data2/\", \"data3/\"], batch_size=32, image_size=(400, 400)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts to torch.Tensor and scales to [0, 1]\n",
    "        transforms.Resize(image_size)  # Resize the image\n",
    "    ])\n",
    "\n",
    "    # Create a list of all possible image paths\n",
    "    all_files = []\n",
    "    for folder_path in folder_paths:\n",
    "        for i in range(len(os.listdir(folder_path))//2):  # Assuming each folder has 4000 images\n",
    "            image_path = os.path.join(folder_path, f\"{i}.png\")\n",
    "            if os.path.exists(image_path):  # Ensure the file exists\n",
    "                all_files.append((folder_path, i))\n",
    "\n",
    "    # Shuffle the list to randomize the order of files\n",
    "    np.random.shuffle(all_files)\n",
    "\n",
    "    images, labels = [], []\n",
    "    for folder_path, i in all_files:\n",
    "        image_path = os.path.join(folder_path, f\"{i}.png\")\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        tensor_image = transform(image)\n",
    "\n",
    "        with open(os.path.join(folder_path, f\"{i}_catalog.pkl\"), \"rb\") as f:\n",
    "            info = pickle.load(f)\n",
    "        labels.append(info[\"coordinate\"] / 5000)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        if len(images) == batch_size:\n",
    "            yield torch.stack(images), torch.tensor(labels)\n",
    "            images, labels = [], []  # Reset for next batch\n",
    "\n",
    "    # Yield any remaining data as the last batch\n",
    "    if images:\n",
    "        yield torch.stack(images), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shihangl/bliss/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.37730324275936855\n",
      "Epoch 2, Loss: 0.7934645481768532\n",
      "Epoch 3, Loss: 0.6772873986404292\n",
      "Epoch 4, Loss: 0.522347771760068\n",
      "Epoch 5, Loss: 0.6082483164441004\n",
      "Epoch 6, Loss: 0.613479896047374\n",
      "Epoch 7, Loss: 0.5292009580426058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute and print loss\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, torch\u001b[38;5;241m.\u001b[39mtensor(output))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bliss/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[162], line 59\u001b[0m, in \u001b[0;36mtest.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 59\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_fc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n",
      "Cell \u001b[0;32mIn[162], line 38\u001b[0m, in \u001b[0;36mtest._forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout25(x)\n",
      "File \u001b[0;32m~/bliss/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bliss/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bliss/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example of using the generator to train the model\n",
    "num_epochs = 10  # Specify the number of epochs\n",
    "batch_size = 32\n",
    "\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(200):\n",
    "    count = 0\n",
    "    for input, output in data_generator():\n",
    "        count += 1\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        # Compute and print loss\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs, torch.tensor(output))\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        if count % 5 == 0:\n",
    "            print(outputs) \n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(data_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming `image_tensor` is your image tensor\n",
    "# Example: image_tensor = torch.rand(3, 900, 900) # Random image for demonstration\n",
    "\n",
    "# Convert the tensor to a PIL Image\n",
    "to_pil = ToPILImage()\n",
    "def visual(batch_images, labels, prediction):\n",
    "    index = random.randint(0, len(batch_images))\n",
    "    plt.imshow(to_pil(batch_images[index]))\n",
    "    plt.scatter(prediction[index][0]*400, prediction[index][1]*400, color='red', s=100, marker='x', label='Predicated Center')\n",
    "    plt.scatter(labels[index][\"coordinate\"][0]/5000*400, labels[index][\"coordinate\"][1]/5000*400, color='green', s=100, marker='x', label='Real Center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visual(\u001b[43mbatch_images\u001b[49m, batch_labels, model(batch_images)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_images' is not defined"
     ]
    }
   ],
   "source": [
    "visual(batch_images, batch_labels, model(batch_images).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be (u, sigma, b1, a1, b2, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_FAVI(\n",
      "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout25): Dropout(p=0.25, inplace=False)\n",
      "  (dropout50): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=73728, out_features=512, bias=True)\n",
      "  (bn_fc): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class test_FAVI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test_FAVI, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)  # Changed from 3 to 4 channels\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout25 = nn.Dropout(0.25)\n",
    "        self.dropout50 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Calculate the correct input size\n",
    "        self.fc1_input_size = self._get_conv_output_size((4, 400, 400))\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 512)  # Adjusted dynamically\n",
    "        self.bn_fc = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout25(x)\n",
    "        \n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout25(x)\n",
    "        \n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout50(x)\n",
    "\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        x = self.dropout50(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        with torch.no_grad():\n",
    "            input = torch.rand(1, *input_size)\n",
    "            output = self._forward_features(input)\n",
    "            return output.data.view(1, -1).size(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "        x = self.dropout50(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = test_FAVI()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(batch_images, labels, prediction):\n",
    "    index = random.randint(0, len(batch_images) - 1)\n",
    "    plt.imshow(to_pil(batch_images[index]))\n",
    "    plt.scatter(prediction[index][0]*400, prediction[index][2]*400, color='red', s=100, marker='x', label='Predicated Center')\n",
    "    plt.scatter(labels[index][0]*400, labels[index][1]*400, color='green', s=100, marker='x', label='Real Center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -1.0812200317723528\n",
      "Epoch 1, Loss: -0.9984853083752623\n",
      "Epoch 1, Loss: -1.1791935820206747\n",
      "Epoch 1, Loss: -1.326569698463933\n",
      "Epoch 1, Loss: -1.0054624565649564\n",
      "tensor([[0.0265, 0.0221, 0.4933, 0.3134],\n",
      "        [0.2804, 0.2692, 0.6386, 0.3267],\n",
      "        [0.1749, 0.2666, 0.4697, 0.6783],\n",
      "        [0.6823, 0.3866, 0.4757, 0.2744],\n",
      "        [0.4215, 0.1983, 0.6098, 0.6217],\n",
      "        [0.4784, 0.2525, 0.6961, 0.0619],\n",
      "        [0.4282, 0.0861, 0.7985, 0.1487],\n",
      "        [0.6121, 0.6922, 0.2143, 0.1414],\n",
      "        [0.6209, 0.2943, 0.6542, 0.0813],\n",
      "        [0.3183, 0.0938, 0.4951, 0.0638],\n",
      "        [0.9009, 0.1761, 0.4740, 0.3798],\n",
      "        [0.4596, 0.2745, 0.7327, 0.1136],\n",
      "        [0.7038, 0.0645, 0.6497, 0.0752],\n",
      "        [0.4600, 0.7654, 0.2716, 0.1656],\n",
      "        [0.2839, 0.1427, 0.5180, 0.2098],\n",
      "        [0.5001, 0.2454, 0.8306, 0.1021],\n",
      "        [0.6989, 0.6772, 0.1427, 0.0091],\n",
      "        [0.5140, 0.6270, 0.5035, 0.5341],\n",
      "        [0.5697, 0.1930, 0.7452, 0.1992],\n",
      "        [0.5350, 0.2438, 0.7111, 0.3605],\n",
      "        [0.3678, 0.0441, 0.4631, 0.0349],\n",
      "        [0.8782, 0.5515, 0.7134, 0.1181],\n",
      "        [0.2180, 0.6124, 0.5071, 0.0255],\n",
      "        [0.4162, 0.1206, 0.6450, 0.6536],\n",
      "        [0.7756, 0.1706, 0.8313, 0.7501],\n",
      "        [0.8507, 0.2894, 0.9092, 0.6354],\n",
      "        [0.7558, 0.1774, 0.2481, 0.0702],\n",
      "        [0.5351, 0.0814, 0.4176, 0.1156],\n",
      "        [0.1965, 0.1476, 0.2482, 0.0034],\n",
      "        [0.8225, 0.3560, 0.5268, 0.0468],\n",
      "        [0.6030, 0.0891, 0.1791, 0.1594],\n",
      "        [0.2844, 0.5354, 0.3813, 0.6331]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3927673/2480257759.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(compare(torch.stack(coordinates), torch.tensor(output)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5556549343791832\n",
      "Epoch 1, Loss: -1.731729518903695\n",
      "Epoch 1, Loss: -1.685470171238377\n",
      "Epoch 1, Loss: -1.628515986913028\n",
      "Epoch 1, Loss: -1.6909663217377138\n",
      "tensor([[0.4586, 0.1593, 0.0974, 0.1701],\n",
      "        [0.4068, 0.2712, 0.7726, 0.7398],\n",
      "        [0.1308, 0.0256, 0.7288, 0.1563],\n",
      "        [0.6843, 0.0498, 0.2279, 0.3012],\n",
      "        [0.4658, 0.0991, 0.8692, 0.1821],\n",
      "        [0.2772, 0.0358, 0.7353, 0.0445],\n",
      "        [0.3252, 0.0437, 0.5975, 0.0719],\n",
      "        [0.6215, 0.1601, 0.4049, 0.3815],\n",
      "        [0.8270, 0.1303, 0.2957, 0.0389],\n",
      "        [0.4288, 0.0776, 0.6686, 0.4082],\n",
      "        [0.6397, 0.1479, 0.7780, 0.4777],\n",
      "        [0.7417, 0.0688, 0.0923, 0.1215],\n",
      "        [0.4483, 0.1536, 0.2799, 0.2557],\n",
      "        [0.4175, 0.1844, 0.8541, 0.1524],\n",
      "        [0.9628, 0.0261, 0.1314, 0.1703],\n",
      "        [0.6590, 0.0876, 0.2200, 0.1755],\n",
      "        [0.5067, 0.6166, 0.0218, 0.0063],\n",
      "        [0.4976, 0.1457, 0.2840, 0.1085],\n",
      "        [0.5991, 0.2518, 0.4334, 0.2248],\n",
      "        [0.0504, 0.1449, 0.8334, 0.0159],\n",
      "        [0.5105, 0.0764, 0.3395, 0.2008],\n",
      "        [0.7441, 0.0185, 0.8397, 0.0172],\n",
      "        [0.5785, 0.0715, 0.3534, 0.0271],\n",
      "        [0.5488, 0.1449, 0.1973, 0.1157],\n",
      "        [0.4310, 0.0948, 0.5155, 0.3359],\n",
      "        [0.5450, 0.0382, 0.8471, 0.0903],\n",
      "        [0.4247, 0.4421, 0.5228, 0.1401],\n",
      "        [0.2343, 0.1397, 0.4648, 0.3867],\n",
      "        [0.3800, 0.0594, 0.4266, 0.0280],\n",
      "        [0.5567, 0.0547, 0.6248, 0.2905],\n",
      "        [0.3337, 0.1737, 0.7081, 0.3145],\n",
      "        [0.7591, 0.0276, 0.8987, 0.0850]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -1.011102465477172\n",
      "Epoch 1, Loss: -2.082308785277193\n",
      "Epoch 1, Loss: -1.8691735927012225\n",
      "Epoch 1, Loss: -2.035664412571757\n",
      "Epoch 1, Loss: -2.1626596981161543\n",
      "tensor([[0.9592, 0.0822, 0.1014, 0.0142],\n",
      "        [0.5051, 0.1596, 0.6422, 0.2114],\n",
      "        [0.6554, 0.1060, 0.3217, 0.1554],\n",
      "        [0.7254, 0.0807, 0.3839, 0.3421],\n",
      "        [0.5621, 0.1587, 0.5012, 0.2081],\n",
      "        [0.0799, 0.0210, 0.5415, 0.0718],\n",
      "        [0.6133, 0.1512, 0.7726, 0.2023],\n",
      "        [0.8098, 0.0261, 0.7872, 0.0363],\n",
      "        [0.4174, 0.1992, 0.3648, 0.4156],\n",
      "        [0.7344, 0.0282, 0.3824, 0.2935],\n",
      "        [0.8210, 0.0550, 0.1943, 0.1911],\n",
      "        [0.0934, 0.0144, 0.8536, 0.3218],\n",
      "        [0.6498, 0.0068, 0.6938, 0.0264],\n",
      "        [0.3974, 0.0663, 0.3341, 0.1001],\n",
      "        [0.1143, 0.0353, 0.5723, 0.1726],\n",
      "        [0.4837, 0.0623, 0.5443, 0.2081],\n",
      "        [0.3500, 0.1760, 0.1718, 0.3454],\n",
      "        [0.5249, 0.1236, 0.4463, 0.2126],\n",
      "        [0.0886, 0.0894, 0.2833, 0.0727],\n",
      "        [0.5020, 0.0613, 0.8251, 0.1051],\n",
      "        [0.7602, 0.1219, 0.3227, 0.1240],\n",
      "        [0.9234, 0.0310, 0.7261, 0.0276],\n",
      "        [0.2189, 0.0241, 0.6276, 0.0923],\n",
      "        [0.8792, 0.0436, 0.1133, 0.0314],\n",
      "        [0.1944, 0.0146, 0.7990, 0.0490],\n",
      "        [0.4834, 0.0705, 0.1248, 0.1475],\n",
      "        [0.3550, 0.0161, 0.8727, 0.4067],\n",
      "        [0.4194, 0.1459, 0.3844, 0.3124],\n",
      "        [0.4014, 0.1253, 0.2967, 0.2879],\n",
      "        [0.3520, 0.0148, 0.8614, 0.0679],\n",
      "        [0.7054, 0.0503, 0.1800, 0.1685],\n",
      "        [0.6202, 0.0655, 0.7874, 0.0654]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1258, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -1.8847653755793208\n",
      "Epoch 1, Loss: -1.823174538506883\n",
      "Epoch 1, Loss: -1.8770122048280173\n",
      "Epoch 1, Loss: -1.3379707569323984\n",
      "Epoch 1, Loss: -1.5314828558312295\n",
      "tensor([[0.6252, 0.0363, 0.4505, 0.0914],\n",
      "        [0.5286, 0.1028, 0.4382, 0.3988],\n",
      "        [0.4738, 0.0398, 0.3311, 0.0356],\n",
      "        [0.3319, 0.0059, 0.1880, 0.0698],\n",
      "        [0.3935, 0.0230, 0.1518, 0.0865],\n",
      "        [0.6732, 0.1139, 0.6032, 0.2673],\n",
      "        [0.2834, 0.0580, 0.6220, 0.2502],\n",
      "        [0.5806, 0.0847, 0.4793, 0.0320],\n",
      "        [0.1142, 0.0425, 0.2973, 0.0751],\n",
      "        [0.4303, 0.0205, 0.8197, 0.1124],\n",
      "        [0.6039, 0.0230, 0.1672, 0.4229],\n",
      "        [0.5018, 0.0385, 0.6964, 0.1404],\n",
      "        [0.6152, 0.0544, 0.2112, 0.0874],\n",
      "        [0.2496, 0.0891, 0.3056, 0.3198],\n",
      "        [0.8276, 0.0427, 0.5515, 0.1431],\n",
      "        [0.3771, 0.0342, 0.4747, 0.1727],\n",
      "        [0.7002, 0.0195, 0.2384, 0.0395],\n",
      "        [0.4577, 0.0235, 0.6677, 0.0557],\n",
      "        [0.8202, 0.0060, 0.8707, 0.0122],\n",
      "        [0.4034, 0.0875, 0.3314, 0.3539],\n",
      "        [0.6389, 0.0767, 0.2208, 0.1902],\n",
      "        [0.2884, 0.0293, 0.1030, 0.1323],\n",
      "        [0.3009, 0.0350, 0.4100, 0.0117],\n",
      "        [0.7572, 0.0681, 0.3982, 0.1817],\n",
      "        [0.9264, 0.0244, 0.8394, 0.0297],\n",
      "        [0.4132, 0.0119, 0.3836, 0.1328],\n",
      "        [0.2973, 0.0513, 0.6871, 0.1969],\n",
      "        [0.4897, 0.0266, 0.7979, 0.0542],\n",
      "        [0.8783, 0.0017, 0.1148, 0.0225],\n",
      "        [0.3427, 0.1434, 0.7138, 0.3624],\n",
      "        [0.6432, 0.0799, 0.5551, 0.2277],\n",
      "        [0.4718, 0.1182, 0.6662, 0.1555]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -1.302056839688733\n",
      "Epoch 1, Loss: -1.6618895576426986\n",
      "Epoch 1, Loss: -1.3418185860835077\n",
      "Epoch 1, Loss: -1.3389995365112792\n",
      "Epoch 1, Loss: -2.044338284381463\n",
      "tensor([[0.5445, 0.0704, 0.3397, 0.2243],\n",
      "        [0.5448, 0.0150, 0.2482, 0.1973],\n",
      "        [0.6867, 0.0328, 0.4537, 0.0998],\n",
      "        [0.8971, 0.0033, 0.4020, 0.0070],\n",
      "        [0.7607, 0.0307, 0.2990, 0.2450],\n",
      "        [0.5148, 0.0540, 0.6978, 0.0318],\n",
      "        [0.5177, 0.0158, 0.3743, 0.0175],\n",
      "        [0.1758, 0.0247, 0.2789, 0.2685],\n",
      "        [0.7310, 0.0302, 0.4871, 0.0842],\n",
      "        [0.6054, 0.1175, 0.3159, 0.1578],\n",
      "        [0.7627, 0.2035, 0.6983, 0.0763],\n",
      "        [0.5597, 0.0616, 0.1536, 0.2085],\n",
      "        [0.2115, 0.0699, 0.7451, 0.0685],\n",
      "        [0.7396, 0.0830, 0.6975, 0.1068],\n",
      "        [0.6077, 0.1919, 0.5765, 0.3136],\n",
      "        [0.5280, 0.0679, 0.6251, 0.1411],\n",
      "        [0.4626, 0.1323, 0.4693, 0.0619],\n",
      "        [0.6026, 0.0701, 0.3696, 0.1139],\n",
      "        [0.4565, 0.0286, 0.6571, 0.0221],\n",
      "        [0.3086, 0.1276, 0.4607, 0.1214],\n",
      "        [0.5598, 0.1029, 0.6558, 0.1054],\n",
      "        [0.8399, 0.0112, 0.3068, 0.0409],\n",
      "        [0.1384, 0.0228, 0.0824, 0.0253],\n",
      "        [0.8292, 0.0016, 0.9149, 0.0163],\n",
      "        [0.5409, 0.0959, 0.3026, 0.1005],\n",
      "        [0.5883, 0.0503, 0.6293, 0.0151],\n",
      "        [0.7357, 0.1794, 0.8258, 0.0810],\n",
      "        [0.1788, 0.0498, 0.2628, 0.1007],\n",
      "        [0.2749, 0.0439, 0.2725, 0.3021],\n",
      "        [0.7434, 0.1347, 0.7118, 0.1646],\n",
      "        [0.4337, 0.0054, 0.1770, 0.0565],\n",
      "        [0.6036, 0.0834, 0.1329, 0.2541]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -1.763932095447085\n",
      "Epoch 1, Loss: -1.828289575863674\n",
      "Epoch 1, Loss: -1.9430235239034888\n",
      "Epoch 1, Loss: -2.1016827648311462\n",
      "Epoch 1, Loss: -1.3144991397571206\n",
      "tensor([[0.5924, 0.4135, 0.7070, 0.1438],\n",
      "        [0.6754, 0.3464, 0.5514, 0.2168],\n",
      "        [0.6895, 0.0825, 0.5329, 0.1786],\n",
      "        [0.2248, 0.0387, 0.6438, 0.0397],\n",
      "        [0.8363, 0.0403, 0.0587, 0.0645],\n",
      "        [0.7320, 0.0453, 0.2635, 0.0473],\n",
      "        [0.6983, 0.0922, 0.8169, 0.0693],\n",
      "        [0.3022, 0.1292, 0.7454, 0.0679],\n",
      "        [0.3783, 0.1739, 0.5118, 0.0674],\n",
      "        [0.1616, 0.0182, 0.1222, 0.0802],\n",
      "        [0.5569, 0.0268, 0.6719, 0.0133],\n",
      "        [0.7406, 0.0244, 0.1208, 0.1320],\n",
      "        [0.5744, 0.2397, 0.7309, 0.1669],\n",
      "        [0.0713, 0.0023, 0.3502, 0.0022],\n",
      "        [0.3876, 0.0351, 0.4040, 0.0473],\n",
      "        [0.6638, 0.0203, 0.2507, 0.0158],\n",
      "        [0.5950, 0.1123, 0.3960, 0.0304],\n",
      "        [0.2964, 0.0083, 0.1338, 0.1593],\n",
      "        [0.5851, 0.3890, 0.6731, 0.2539],\n",
      "        [0.6884, 0.0758, 0.6294, 0.1337],\n",
      "        [0.5961, 0.1225, 0.3133, 0.2410],\n",
      "        [0.4950, 0.0172, 0.4082, 0.0039],\n",
      "        [0.6838, 0.0526, 0.0688, 0.2157],\n",
      "        [0.5170, 0.1869, 0.4705, 0.3075],\n",
      "        [0.5964, 0.3353, 0.7327, 0.0758],\n",
      "        [0.5599, 0.0953, 0.5267, 0.1992],\n",
      "        [0.5620, 0.1587, 0.8633, 0.0228],\n",
      "        [0.7613, 0.0624, 0.8147, 0.0846],\n",
      "        [0.3597, 0.3406, 0.4604, 0.0867],\n",
      "        [0.5362, 0.2185, 0.7259, 0.2690],\n",
      "        [0.7943, 0.2225, 0.6599, 0.0599],\n",
      "        [0.8217, 0.0057, 0.0408, 0.0044]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1545, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -2.0817614435072294\n",
      "Epoch 1, Loss: -1.7651667421799397\n",
      "Epoch 1, Loss: -2.3117839842286143\n",
      "Epoch 1, Loss: -1.7748104747241045\n",
      "Epoch 1, Loss: -1.938811438943091\n",
      "tensor([[0.7890, 0.0194, 0.6785, 0.0057],\n",
      "        [0.7843, 0.2306, 0.5460, 0.0702],\n",
      "        [0.7939, 0.2246, 0.4891, 0.3339],\n",
      "        [0.2903, 0.0103, 0.2221, 0.1061],\n",
      "        [0.7350, 0.0819, 0.2739, 0.1811],\n",
      "        [0.4064, 0.0245, 0.5252, 0.0327],\n",
      "        [0.7609, 0.0824, 0.3153, 0.0592],\n",
      "        [0.5725, 0.0522, 0.6955, 0.0077],\n",
      "        [0.5869, 0.1668, 0.5083, 0.1422],\n",
      "        [0.3940, 0.0539, 0.5834, 0.0200],\n",
      "        [0.7414, 0.3357, 0.4486, 0.1544],\n",
      "        [0.4026, 0.0158, 0.5473, 0.0329],\n",
      "        [0.4064, 0.1958, 0.7419, 0.0365],\n",
      "        [0.4768, 0.0671, 0.0946, 0.1124],\n",
      "        [0.7413, 0.0743, 0.5014, 0.0185],\n",
      "        [0.7137, 0.2993, 0.7906, 0.1807],\n",
      "        [0.4940, 0.2480, 0.5634, 0.1797],\n",
      "        [0.8542, 0.2847, 0.5383, 0.0098],\n",
      "        [0.4672, 0.0495, 0.0769, 0.0838],\n",
      "        [0.6552, 0.5194, 0.7382, 0.0346],\n",
      "        [0.0616, 0.0405, 0.4260, 0.0644],\n",
      "        [0.7845, 0.0293, 0.5447, 0.0219],\n",
      "        [0.6338, 0.2151, 0.4050, 0.2590],\n",
      "        [0.5685, 0.1367, 0.6275, 0.1760],\n",
      "        [0.6424, 0.3904, 0.7396, 0.0457],\n",
      "        [0.3665, 0.0039, 0.3936, 0.0221],\n",
      "        [0.7863, 0.0731, 0.1217, 0.2369],\n",
      "        [0.5321, 0.1021, 0.3986, 0.1423],\n",
      "        [0.4837, 0.1484, 0.5670, 0.0518],\n",
      "        [0.2907, 0.0957, 0.4070, 0.1136],\n",
      "        [0.5540, 0.1332, 0.2617, 0.4784],\n",
      "        [0.4664, 0.3627, 0.8253, 0.0765]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -2.0482498648904466\n",
      "Epoch 1, Loss: -2.242238649282601\n",
      "Epoch 1, Loss: -1.8635348960801772\n",
      "Epoch 1, Loss: -1.6981801157906256\n",
      "Epoch 1, Loss: -1.8893357209535488\n",
      "tensor([[0.6724, 0.2045, 0.6557, 0.0687],\n",
      "        [0.6777, 0.3242, 0.9468, 0.0059],\n",
      "        [0.5518, 0.1674, 0.7108, 0.1366],\n",
      "        [0.0589, 0.0667, 0.3391, 0.0435],\n",
      "        [0.7665, 0.2050, 0.6595, 0.0914],\n",
      "        [0.7306, 0.0808, 0.7608, 0.1155],\n",
      "        [0.3344, 0.1894, 0.4949, 0.1468],\n",
      "        [0.4804, 0.0931, 0.4754, 0.1525],\n",
      "        [0.6639, 0.0029, 0.0313, 0.0063],\n",
      "        [0.7610, 0.2374, 0.4087, 0.0662],\n",
      "        [0.2851, 0.3108, 0.7569, 0.0937],\n",
      "        [0.3222, 0.0605, 0.5023, 0.0384],\n",
      "        [0.1224, 0.0440, 0.2493, 0.0354],\n",
      "        [0.7116, 0.1896, 0.7277, 0.1253],\n",
      "        [0.4831, 0.1876, 0.5211, 0.1845],\n",
      "        [0.4822, 0.1972, 0.5666, 0.1691],\n",
      "        [0.7412, 0.0471, 0.3626, 0.0634],\n",
      "        [0.6448, 0.0902, 0.3081, 0.2006],\n",
      "        [0.5441, 0.2036, 0.6429, 0.0511],\n",
      "        [0.8260, 0.0984, 0.3286, 0.1074],\n",
      "        [0.2653, 0.2147, 0.7006, 0.1246],\n",
      "        [0.8319, 0.0068, 0.2596, 0.0514],\n",
      "        [0.6550, 0.0807, 0.4836, 0.0719],\n",
      "        [0.7844, 0.0676, 0.1055, 0.1500],\n",
      "        [0.6099, 0.0980, 0.6098, 0.0461],\n",
      "        [0.8489, 0.1104, 0.4556, 0.0275],\n",
      "        [0.5377, 0.1002, 0.4774, 0.0878],\n",
      "        [0.1395, 0.1816, 0.5062, 0.0121],\n",
      "        [0.4534, 0.0257, 0.3776, 0.0950],\n",
      "        [0.4685, 0.0495, 0.6589, 0.0158],\n",
      "        [0.5939, 0.0749, 0.3391, 0.1689],\n",
      "        [0.3487, 0.3571, 0.8551, 0.0406]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -1.8627830772940541\n",
      "Epoch 1, Loss: -1.5054228969938002\n",
      "Epoch 1, Loss: -2.000857735019308\n",
      "Epoch 1, Loss: -2.006040242683552\n",
      "Epoch 1, Loss: -2.2324061520867073\n",
      "tensor([[0.6986, 0.0485, 0.8239, 0.0432],\n",
      "        [0.2883, 0.4160, 0.9236, 0.0245],\n",
      "        [0.8113, 0.0435, 0.7058, 0.0130],\n",
      "        [0.6213, 0.0704, 0.8682, 0.0808],\n",
      "        [0.4925, 0.2453, 0.6543, 0.1195],\n",
      "        [0.1397, 0.1858, 0.3433, 0.0590],\n",
      "        [0.6860, 0.0238, 0.5067, 0.0404],\n",
      "        [0.5324, 0.1059, 0.2066, 0.1091],\n",
      "        [0.8808, 0.0735, 0.6347, 0.0293],\n",
      "        [0.7617, 0.0234, 0.3301, 0.0420],\n",
      "        [0.3981, 0.1190, 0.5286, 0.1409],\n",
      "        [0.2222, 0.1144, 0.4364, 0.1497],\n",
      "        [0.3608, 0.0498, 0.3714, 0.0114],\n",
      "        [0.5081, 0.1762, 0.2731, 0.2149],\n",
      "        [0.8228, 0.0433, 0.2114, 0.0938],\n",
      "        [0.3835, 0.2177, 0.7473, 0.0993],\n",
      "        [0.2943, 0.2465, 0.8029, 0.0491],\n",
      "        [0.6308, 0.5068, 0.8535, 0.0403],\n",
      "        [0.0520, 0.0206, 0.1361, 0.0630],\n",
      "        [0.5102, 0.4408, 0.6825, 0.1583],\n",
      "        [0.1039, 0.0408, 0.0763, 0.0725],\n",
      "        [0.7526, 0.0089, 0.1026, 0.0385],\n",
      "        [0.7842, 0.0240, 0.2030, 0.0221],\n",
      "        [0.6312, 0.0719, 0.3656, 0.1404],\n",
      "        [0.4873, 0.1280, 0.6826, 0.0626],\n",
      "        [0.7522, 0.0934, 0.5290, 0.1433],\n",
      "        [0.0919, 0.0743, 0.5498, 0.0656],\n",
      "        [0.5027, 0.0298, 0.2290, 0.1397],\n",
      "        [0.5228, 0.0467, 0.3901, 0.0322],\n",
      "        [0.5826, 0.1970, 0.3410, 0.1859],\n",
      "        [0.0989, 0.0439, 0.0798, 0.0861],\n",
      "        [0.4973, 0.0351, 0.1019, 0.0776]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1146, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -2.1443566394047155\n",
      "Epoch 1, Loss: -2.2080790099255125\n",
      "Epoch 1, Loss: -1.636827253216416\n",
      "Epoch 1, Loss: -1.8265066811619088\n",
      "Epoch 1, Loss: -2.3042858972474587\n",
      "tensor([[0.4433, 0.2212, 0.5809, 0.0972],\n",
      "        [0.4788, 0.5183, 0.7324, 0.1877],\n",
      "        [0.4850, 0.0864, 0.3093, 0.1389],\n",
      "        [0.3528, 0.4445, 0.8911, 0.0560],\n",
      "        [0.3212, 0.1890, 0.5781, 0.1463],\n",
      "        [0.3868, 0.0926, 0.6695, 0.0304],\n",
      "        [0.5780, 0.1105, 0.5716, 0.0558],\n",
      "        [0.2944, 0.0429, 0.1234, 0.0832],\n",
      "        [0.4742, 0.1929, 0.4409, 0.2073],\n",
      "        [0.7903, 0.0536, 0.3540, 0.0398],\n",
      "        [0.5447, 0.0270, 0.1427, 0.0848],\n",
      "        [0.4037, 0.1052, 0.3872, 0.0243],\n",
      "        [0.4012, 0.2161, 0.6162, 0.1026],\n",
      "        [0.3110, 0.1043, 0.3459, 0.0729],\n",
      "        [0.8582, 0.0149, 0.8104, 0.0209],\n",
      "        [0.1673, 0.0995, 0.3181, 0.0346],\n",
      "        [0.4144, 0.1285, 0.1735, 0.0584],\n",
      "        [0.8344, 0.0136, 0.4487, 0.0202],\n",
      "        [0.4756, 0.1377, 0.7633, 0.0405],\n",
      "        [0.5340, 0.0254, 0.1848, 0.1019],\n",
      "        [0.7174, 0.0509, 0.8034, 0.1585],\n",
      "        [0.6856, 0.1679, 0.6804, 0.0970],\n",
      "        [0.6979, 0.3586, 0.6581, 0.2271],\n",
      "        [0.1126, 0.1908, 0.1521, 0.0421],\n",
      "        [0.7784, 0.0776, 0.6568, 0.0848],\n",
      "        [0.6371, 0.0560, 0.2859, 0.1512],\n",
      "        [0.2581, 0.1803, 0.7400, 0.0510],\n",
      "        [0.8319, 0.0058, 0.3656, 0.0273],\n",
      "        [0.3976, 0.1344, 0.4164, 0.1970],\n",
      "        [0.1997, 0.1613, 0.6723, 0.1030],\n",
      "        [0.8325, 0.0153, 0.4405, 0.0151],\n",
      "        [0.5785, 0.1635, 0.8342, 0.1372]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.1037, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Loss: -2.2828730586962207\n",
      "Epoch 1, Loss: -2.3934062454087197\n",
      "Epoch 1, Loss: -2.3562590296772203\n",
      "Epoch 1, Loss: -2.4032999015537406\n",
      "Epoch 1, Loss: -2.2963624343744335\n",
      "tensor([[0.7386, 0.1511, 0.2884, 0.0407],\n",
      "        [0.7806, 0.0619, 0.8917, 0.0379],\n",
      "        [0.5176, 0.1872, 0.4542, 0.2105],\n",
      "        [0.8632, 0.0148, 0.7572, 0.0113],\n",
      "        [0.7590, 0.0729, 0.4842, 0.0220],\n",
      "        [0.7135, 0.0308, 0.7434, 0.0571],\n",
      "        [0.2656, 0.1471, 0.1701, 0.0434],\n",
      "        [0.