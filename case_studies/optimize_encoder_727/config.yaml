---
defaults:
    - ../../conf@_here_: base_config
    - _self_
    - override hydra/job_logging: stdout

hydra:
    sweeper:
        params:
            training.seed: 15942,17508,16433,2857,25956
            # cached_simulator.batch_size: 8, 16, 32, 64, 128, 256
            # encoder.architecture.anchors: [[4, 4]], [
            #         [10,13, 16,30, 33,23]  # P3/8
            #         [30,61, 62,45, 59,119]  # P4/16
            #         [116,90, 156,198, 373,326]  # P5/32
            #     ], 1, 2, 3, 4
            # encoder.architecture.anchors: "[[2, 2]], [[4, 4]], [[8, 8]], [[12, 12]], [[16, 16]], [[20, 20]]"
            # encoder.architecture.head: [ # base_config (YOLOv5)
            #         [-1, 1, Conv, [512, 1, 1]],
            #         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            #         [[-1, 6], 1, Concat, [1]],
            #         [-1, 3, C3, [512, false]],
            #         [-1, 1, Conv, [256, 1, 1]],
            #         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            #         [[-1, 4, 5], 1, Concat, [1]],
            #         [-1, 3, C3, [256, false]],
            #         [[17], 1, Detect, [nc, anchors]]
            #     ], [ # YOLOv8
            #         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            #         [[-1, 6], 1, Concat, [1]]  # cat backbone P4,
            #         [-1, 3, C2f, [512]]  # 12,
            #         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
            #         [[-1, 4], 1, Concat, [1]]  # cat backbone P3,
            #         [-1, 3, C2f, [256]]  # 15 (P3/8-small),
            #         [-1, 1, Conv, [256, 3, 2]],
            #         [[-1, 12], 1, Concat, [1]]  # cat head P4,
            #         [-1, 3, C2f, [512]]  # 18 (P4/16-medium),
            #         [-1, 1, Conv, [512, 3, 2]],
            #         [[-1, 9], 1, Concat, [1]]  # cat head P5,
            #         [-1, 3, C2f, [1024]]  # 21 (P5/32-large),
            #         [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
            #     ],
            # encoder.scheduler_params:
            #     # ----- MultiStepLR -----
            #     # milestones: [32]
            #     # gamma: 0.1
            #     # ----- OneCycleLR -----
            #     max_lr: 1e-3 
            # encoder.tiles_to_crop: 1, 2


paths:
    project: ${paths.root}/case_studies/optimize_encoder_727
    output: ${paths.project}/output
    pretrained_models: ${paths.data}/pretrained_models

simulator:
    prior:
        mean_sources: 0.006
    background:
        _target_: bliss.simulator.background.SimulatedSDSSBackground
        sdss_dir: ${paths.sdss}
        run: 94
        camcol: 1
        field: 12
        bands: [2]

cached_simulator:
    batch_size: 64
    # 59GB dataset - 140 files
    #  training set  : 112 files * 96 batches * 64 images/batch = 688,128 images (80%)
    #  validation set: 14 files * 10 batches * 64 images/batch = 8,960 images (10%)
    #  testing set   : 14 files * 10 batches * 64 images/batch = 8,960 images (10%)
    train_n_batches: 10752
    val_split_file_idxs: [113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]
    test_split_file_idxs: null # [127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]
    cached_data_path: /data/scratch/zhteoh/cached_dataset_${simulator.prior.mean_sources}_50GB

generate:
    n_batches: 32
    batch_size: 128
    max_images_per_file: 1024
    cached_data_path: /data/scratch/zhteoh/cached_dataset_large_0.06

training:
    name: "encoder"
    version: "sdss-pretrained-50GB-${simulator.prior.mean_sources}-val-per-epoch"
    enable_early_stopping: false
    pretrained_weights: ${paths.pretrained_models}/sdss-pretrained-50GB-${simulator.prior.mean_sources}.pt
    trainer:
        check_val_every_n_epoch: 1
    weight_save_path: ${paths.output}/${training.name}/${training.version}.pt
    seed: 42
    use_cached_simulator: true
